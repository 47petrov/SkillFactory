# -*- coding: utf-8 -*-
"""Юнит-3_Проект 3. О вкусной и здоровой пище

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WIUKk9sEr2D93Xma40pdBuC9LN2xn96o

**ПРОЕКТ 3: О вкусной и здоровой пище**

# ЧТО ВЫ ПОЛУЧИТЕ В РЕЗУЛЬТАТЕ РАБОТЫ?

- Cоздадите свою первую модель, основанную на алгоритмах машинного обучения.
- Примете участие в соревновании на kaggle.
- Поймёте, как правильно «готовить» данные, чтобы ваша модель работала лучше.

# Что нужно сделать?
→ разобраться с подробным описанием того, как будет организовано изучение модуля;

→ зарегистрироваться в соревновании на kaggle;

→ проанализировать baseline к соревнованию;

→ следуя подсказкам в модуле, обработать оставшиеся признаки и подготовить собственное решение;

→ победить в соревновании :)

# Постановка задачи проекта 3:

Представьте, что вы работаете DS в компании TripAdvisor. 

Одна из проблем компании — это нечестные рестораны, которые накручивают себе рейтинг. Одним из способов нахождения таких ресторанов является построение модели, которая предсказывает рейтинг ресторана. 

Если предсказания модели сильно отличаются от фактического результата, то, возможно, ресторан играет нечестно, и его стоит проверить.

Вам поставлена задача создать такую модель.

# ШАГ 1. Разбираемся с данными

**Что мы сделаем на этом шаге?**

→ Посмотрим на данные.

→ Посмотрим, где у нас пропуски.

→ Определим тип признаков.

**На самом деле всё предельно просто:** 

в этом модуле вы будете работать с датасетом, содержащим сведения о 40 000 ресторанах Европы, а модель, которую вы будете обучать, должна будет предсказывать рейтинг ресторана по данным сайта TripAdvisor на основе имеющихся в датасете данных.

**Структура датасета:**

Первоначальная версия датасета состоит из десяти столбцов, содержащих следующую информацию:

**Restaurant_id** — идентификационный номер ресторана / сети ресторанов;

**City** — город, в котором находится ресторан;

**Cuisine Style** — кухня или кухни, к которым можно отнести блюда, предлагаемые в ресторане;

**Ranking** — место, которое занимает данный ресторан среди всех ресторанов своего города;

**Rating** — рейтинг ресторана по данным TripAdvisor (именно это значение должна будет предсказывать модель);

**Price Range** — диапазон цен в ресторане;

**Number of Reviews** — количество отзывов о ресторане;

**Reviews** — данные о двух отзывах, которые отображаются на сайте ресторана;

**URL_TA** — URL страницы ресторана на TripAdvisor;

**ID_TA** — идентификатор ресторана в базе данных TripAdvisor.
"""

# Commented out IPython magic to ensure Python compatibility.
# Загрузка библиотек:

import pandas as pd
import numpy as np
import ast
import datetime as dt
import io

import matplotlib.pyplot as plt
import seaborn as sns 
# %matplotlib inline

from datetime import datetime, date, time, timedelta
from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели  
from sklearn import metrics # инструменты для оценки точности модели  

# Загружаем специальный инструмент для разбивки:  
from sklearn.model_selection import train_test_split

# Иструмент для кодирования
from sklearn.preprocessing import LabelEncoder
import re

# Загрузка классов из библиотек

from pprint import pprint

pd.set_option('display.max_columns', None)

# Загрузка датасета размером 40000 записей:

data = pd.read_csv('main_task_new.csv')
data.info()

"""# ШАГ 2. Машинное обучение для чайников

**ОРГАНИЗАЦИОННАЯ ИНФОРМАЦИЯ**

Что нужно сделать на этом шаге?
Создать свою первую модель, основанную на алгоритмах машинного обучения. Код для создания модели мы предоставим, но чтобы модель заработала, в неё нужно загрузить правильные данные. Приведение данных в нужный вид и будет вашей задачей.

**1 РАЗДЕЛЕНИЕ ДАТАФРЕЙМА**

Прежде всего, для создания модели необходимо разделить датафрейм на набор данных, которые мы будем использовать для обучения модели (), и на целевую переменную, т.е. величину, значение которой мы будем предсказывать ().

Далее каждый из полученных наборов мы делим на тренировочный (train, используется для обучения модели) и тестовый (test, используется для оценки точности модели). Такое деление осуществляется с помощью специального метода, входящего в состав библиотеки Scikit-Learn (sklearn). В параметрах метода мы указываем, какую часть исходного датафрейма нужно оставить для тестирования модели. В нашем коде эта часть составляет 25%, или 0.25.
"""

# Разбиваем датафрейм на части, необходимые для обучения и тестирования модели  
# Х - данные с информацией о ресторанах, у - целевая переменная (рейтинги ресторанов)  
df=data.copy(deep=True)

y = df['Rating'] 
X = df.drop(['Restaurant_id', 'Rating'], axis = 1)  
#y = df['Rating']  

X.info()
print('Размер вектора у=', len(y))

# Загружаем специальный инструмент для разбивки:  
from sklearn.model_selection import train_test_split  
      
# Наборы данных с меткой "train" будут использоваться для обучения модели, "test" - для тестирования.  
# Для тестирования мы будем использовать 25% от исходного датасета.  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

"""**2 СОЗДАНИЕ, ОБУЧЕНИЕ И ТЕСТИРОВАНИЕ МОДЕЛИ**

Сам процесс создания и тестирования модели занимает всего четыре строчки кода (при условии, что мы используем параметры по умолчанию):
"""

# Импортируем необходимые библиотеки:  
from sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели  
from sklearn import metrics # инструменты для оценки точности модели  
      
# Создаём модель  
regr = RandomForestRegressor(n_estimators=100)  
      
# Обучаем модель на тестовом наборе данных  
regr.fit(X_train, y_train)  
      
# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.  
# Предсказанные значения записываем в переменную y_pred  
y_pred = regr.predict(X_test)

"""для корректной работы все данные в датафрейме, который вы будете использовать при обучении модели, должны быть в форматах int или float. 

Есть и ещё одно жёсткое ограничение: в столбцах не должно быть None-значений. В реальных проектах в индустрии используются разные подходы к работе с None-значениями. Если позволяют условия, то строки, содержащие неполные данные, просто удаляют из датасета. 

Однако **в задачах по машинному обучению эта стратегия не используется никогда**, потому что настоящие герои никогда не ищут лёгких путей :) Вместо каждого NaN вам нужно будет вычислить и поместить в ячейку максимально близкое к реальности значение.

**! чтобы код, отвечающий за обучение модели, заработал, мы должны использовать в нём датафрейм, содержащий только количественные признаки и не содержащий None-значений!**

**На первом этапе**  для создания такого датафрейма давайте просто удалим столбцы, содержащие данные типа object, и заполним пропущенные значения (None или NaN) каким-то одним значением (нулём или средним арифметическим) для всего столбца.

**Для выполнения этих операций** вам понадобятся следующие методы библиотеки Pandas:
- **drop** — метод для удаления ненужных строк и столбцов (обратите внимание, что в материалах данного модуля метод разрешается использовать только для удаления столбцов);
- **fillna** — метод для заполнения пропущенных значений в столбце или во всём датафрейме.
"""

# удалим столбцы, содержащие данные типа object  

X = X.drop(['City', 'Cuisine Style', 'Price Range', 'Reviews', 'URL_TA', 'ID_TA'], axis = 1)  

X.info()

# заполним пропущенные значения (None или NaN) 
# каким-то одним значением (нулём или средним арифметическим)
# для всего столбца

X = X.fillna(value=0)
X.info()

# Загружаем специальный инструмент для разбивки:  
from sklearn.model_selection import train_test_split  
      
# Наборы данных с меткой "train" будут использоваться для обучения модели, "test" - для тестирования.  
# Для тестирования мы будем использовать 25% от исходного датасета.  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

# Импортируем необходимые библиотеки:  
from sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели  
from sklearn import metrics # инструменты для оценки точности модели  
      
# Создаём модель  
regr = RandomForestRegressor(n_estimators=100)  
      
# Обучаем модель на тестовом наборе данных  
regr.fit(X_train, y_train)  
      
# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.  
# Предсказанные значения записываем в переменную y_pred  
y_pred = regr.predict(X_test) 
y_pred

"""**КАК УЗНАТЬ, ЧТО МОЯ МОДЕЛЬ ХОРОШАЯ?**

**Для оценки точности прогнозов**, сделанных моделью, мы будем использовать метрику (показатель), которая называется Mean Absolute Error, и представляет собой среднее абсолютное значение отклонения предсказанных значений от фактических:

**Для оценки точности прогнозов**, сделанных моделью, мы будем использовать метрику (показатель), которая называется **Mean Absolute Error**, и представляет собой **среднее абсолютное значение отклонения** предсказанных значений от фактических:
"""

# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются  
# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.  
print('MAE:', round(metrics.mean_absolute_error(y_test, y_pred),2))

"""# ШАГ 3. Сделайте с этим что-нибудь!

**ОРГАНИЗАЦИОННАЯ ИНФОРМАЦИЯ**

**Что нужно сделать на этом шаге?**

Повысить точность прогнозов, которые делает модель, с помощью более тщательной подготовки данных. При подготовке датасета нужно будет не только избавиться от пропущенных (None) значений и нечисловых признаков, но и сгенерировать новые признаки (добавить в датафрейм новые столбцы) на основе информации, уже содержащейся в данных.

**А ЧТО ДЕЛАТЬ-ТО?**

С одной стороны, всё просто. Задачу, которая стоит перед вами, можно свести к трём пунктам:

- Удалить из датафрейма столбцы, данные в которых представлены не числами (это вы уже сделали, и нужно просто повторить знакомые действия, но в этот раз выполнить данный шаг в последнюю очередь).

- Избавиться от пропущенных (None) значений (на предыдущем шаге мы делали это самым грубым из всех возможных способов; сейчас попробуем подойти к процессу более гибко).

- Создать новые столбцы с данными, используя для этого информацию, содержащуюся в других столбцах датафрейма (например, можно добавить столбец, сообщающий, сколько дней прошло со дня публикации последнего отзыва, отображённого на сайте).

⛔ С другой стороны, в этом задании масса подводных камней.

**Строковые данные**

В исходном наборе данных всего три столбца содержат числовые данные, причём один из этих столбцов — это целевая переменная, значение которой должна предсказывать наша модель. Так что просто удалить все object-значения и считать задачу выполненной не получится. Для создания качественной модели нам сначала придётся очень основательно поработать со строковыми данными и извлечь из них как можно больше информации, которую можно представить в числовом виде.

**Пропущенные значения**

Мы уже говорили о том, что в задачах по машинному обучению принято не удалять строки с пустыми значениями, а заполнять их максимально близкими к реальности данными. Как найти такие данные — большой вопрос, требующий не только знания синтаксиса, но и креативности, изобретательности, хорошего понимания контента, а иногда ещё и интуиции.

**Новые признаки**

Это, пожалуй, самая сложная, но и самая интригующая, увлекательная и творческая часть работы на данном шаге. Создание новых признаков (Feature Engineering) потребует от вас не только хорошего владения разными библиотеками Python, но и способность вникать в контент, умение подключать к работе интуицию, творческий подход и готовность к экспериментам.

**ПОДСКАЗКИ БЕЗ ПОДСКАЗОК**

По условию нашего курса при выполнении этого задания у каждого студента должен получиться свой уникальный датафрейм, содержащий уникальный набор признаков. Иными словами, мы не стремимся привести всех к единому результату и одинаковому «единственно правильному» набору признаков. В то же время у нас есть определённое представление о том, какие признаки точно можно сгенерировать на основе набора данных, который вы получили для работы.

**4.1 Вопросы о ценах**

Сколько вариантов непустых значений встречается в **столбце Price Range**?
"""

data = pd.read_csv('main_task_new.csv')
data.info()

data['Price Range'].describe() # ответ = 3

data['Price Range'].isnull()

"""**Как в датафрейме обозначается самый низкий уровень цен?**"""

data['Price Range'].sort_values(ascending=True).head(2) # ответ = $

"""**Как в датафрейме обозначается самый высокий уровень цен?**"""

data['Price Range'].sort_values(ascending=False).head(2) # ответ = $$$$

"""**Сколько ресторанов относятся к среднему ценовому сегменту?**"""

data['Price Range'].describe() # ответ = 18412

"""**4.2 Вопрос о городах**

Сколько городов представлено в наборе данных?
"""

data['City'].describe() # ответ = 31

"""**4.3 Вопросы о кухнях**

Сколько типов кухонь представлено в наборе данных?
"""

df=data.copy(deep=True)

df['Cuisine Style'].value_counts().head(5) # ответ = 125

df.rename(columns={'Cuisine Style':'Cuisine_style'}, inplace=True)
df.head(1)

# очистка исходного датасета в признаке 'Cuisine Style'
# разделение списков на отдельные строки
# если это сделать, то размерность датасета увеличится
# пока увеличивать размерность не будем

# переименование признака 'Cuisine Style'
# далее
# очистка от символов и разделение на отдельные значения

# не используем # вариант 1
def clean_name(str_val):
 
    if pd.isna(str_val): return ["Unknown"]
    str_val = str_val.strip('[]')
    str_val = str_val.replace("\'",'')
#    str_val = str_val.replace("(\s+|\s+$)", '')
    str_val = str_val.split(',')
    return str_val

# вызов функции clean_name
# для очистки признака 'Сuisine_style'

df['Cuisine_style'] = df['Cuisine_style'].apply(clean_name)
df['Cuisine_style'].value_counts()

# не используем #вариант 2
##def cuisine_count(x):
##    try:
##        return len(x.split(','))
##    except:
##        return x

##df['Cuisine_Count'] = df['Cuisine_style'].apply(cuisine_count)
##df['Cuisine_Count'].value_counts()

# не используем # Вариант 3
df['Cuisine_style'] = df['Cuisine_style'].apply(lambda x: str(x).replace('[', ''))
df['Cuisine_style'] = df['Cuisine_style'].apply(lambda x: str(x).replace(']', ''))
df['Cuisine_style'] = df['Cuisine_style'].apply(lambda x: str(x).replace(' ', ''))

def data_explode(df, col, cnt = False):
    df[col] = df[col].str.split(',')
    df = df.explode(col)
    if cnt:
        return df[col].nunique()#.value_counts()
    return df

df_1 = df.copy()
df_1 = data_explode(df_1, 'Cuisine_style', cnt = True)
df_1

# ИСПОЛЬЗУЕМ ВАРИАНТ 4

# Вариант 4: даёт верный результат
# но метод explode позволяет сделать только подсчет
# без сохранения результатов в датасете

def to_list(x):
   if pd.isna(x)==True:
       return np.nan
   else:
       x=x.strip("[]")
       x=x.strip(" ")       
       y=x.split(", ")
       return y

df_1 = df.copy(deep=True)
df_1["Cuisine_style"]=df_1["Cuisine_style"].apply(to_list)

print('ответ: ', df_1["Cuisine_style"].explode().nunique())

"""**Какая кухня представлена в наибольшем количестве ресторанов? Введите название кухни без кавычек или апострофов.**"""

print('ответ: ', df_1["Cuisine_style"].explode().value_counts()) # ответ = Vegetarian Friendly

"""**Какое среднее количество кухонь предлагается в одном ресторане? Если в данных отсутствует информация о типах кухонь, то считайте, что в этом ресторане предлагается только один тип кухни. Ответ округлите до одного знака после запятой.**"""

grouped_CS = df_1.groupby(['Restaurant_id'])['Cuisine_style'].count().mean()  # ответ = 2.6
display('ответ:', round(grouped_CS,1))

"""**4.4 Вопросы об отзывах**

ЗДЕСЬ создаем фрагмент кода для очистки данных в признаки Reviews (самый "грязный" признак; есть текстовый комментарий, есть даты, в каждой ячейке признака список; т.е. получается  в признаке список с вложенным списком. Да еще не полноценным: в общем, размер вложенного списка должен быть list[0,0,0,0])

**Когда был оставлен самый свежий отзыв? Введите ответ в формате yyyy-mm-dd.**
"""

# отображаем формат даты, записанный в признаке Reviews
# типовой формат '11/18/2017', '02/19/2017' mm/dd/yyyy

# [['Best place to try a Bavarian food', 'Nice building but improvements need to be...'], ['11/18/2017', '02/19/2017']]

# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:
!pip freeze > requirements.txt

import pandas as pd
import numpy as np
import ast
import datetime as dt
import io

from datetime import datetime, date, time, timedelta
from sklearn.model_selection import train_test_split

# Иструмент для кодирования
from sklearn.preprocessing import LabelEncoder
import re

# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!
RANDOM_SEED = 42

data= pd.read_csv('main_task_new.csv')
data.info()

# делаем копию датасета и работаем далее с копией
# очищаем признак от пробелов
# разделяем на части
# разделителем - знак ","

df_4=data.copy(deep=True)
df_4.head(1)

"""**ПРИЗНАКИ и ЗНАЧЕНИЯ ПРИЗНАКОВ в ДАТАСЕТЕ:**

Restaurant_id	City	Cuisine Style	Ranking	Rating	Price Range	Number of Reviews	Reviews	URL_TA	ID_TA

0	id_5569	Paris	['European', 'French', 'International']	5570.0	3.5	$$ - $$$	194.0	[['Good food at your doorstep', 'A good hotel ...	/Restaurant_Review-g187147-d1912643-Reviews-R_...	d1912643
"""

df_4['Reviews'][0]

# '12\/31\/2017', '11\/20\/2017'

# Пропуски известны, посчитаем кол-во в колонках и после ознакомления с данными решим что делать.

df_4['Reviews'].isnull().sum()

df_4['Reviews'].value_counts()

type(df_4['Reviews'][1])

df_4['Reviews'] = df_4['Reviews'].fillna('[[], []]')
df_4['NAN_reviews'] = (df_4['Reviews'] == '[[], []]').astype('uint8')

def processing_reviews(df):
    """
    Функция разбирает строку с комментарием и датой 
    и помещает комментарии и даты в 4 новые колонки 
    2 для комментариев и 2 для дат
    
    """
    
    
    reviews_fb_1 = []
    reviews_fb_2 = []
    reviews_date_1 = []
    reviews_date_2 = []
    
    for index, row in df.iterrows():
        str_review = ast.literal_eval(str(row['Reviews'].replace('nan','0')))   # Разбили строку с комментарием на пополам (комменты/даты)
        
        if str_review == [[], []]:
            reviews_fb_1.append('')
            reviews_fb_2.append('')
            reviews_date_1.append('undefined')
            reviews_date_2.append('undefined')
        else:
            reviews_fb_1.append(str_review[0][0])
            reviews_date_1.append(datetime.strptime(str_review[1][0],'%m/%d/%Y')) 
            if len(str_review[0]) == 1:
                reviews_fb_2.append('')
                reviews_date_2.append('undefined')
            else:
                reviews_fb_2.append(str_review[0][1])
                reviews_date_2.append(datetime.strptime(str_review[1][1],'%m/%d/%Y'))

    df['reviews_fb_1'] = reviews_fb_1
    df['reviews_fb_2'] = reviews_fb_2
    df['date_1'] = reviews_date_1
    df['date_2'] = reviews_date_2


processing_reviews(df_4)

df_4.head(1)

df_4.date_1.value_counts().tail(10)

df_4.date_2.value_counts().tail(10)

"""# Ответ на задание 4.4.1:

**Когда был оставлен самый свежий отзыв? Введите ответ в формате yyyy-mm-dd.**
"""

print(df_4.date_1.dtype,'/', df_4.date_2.dtype)

type(df_4.date_1)

df_4.insert(15, "min_date_review",'')
df_4.head(1)

# считаем дни: первый и последний для каждой ячейки
# определяем количество дней между отзывами и текущей датой
# определяем ближайшую дату к сегодняшнему дню

for ind in range (len(df_4)):

  day_1=df_4['date_1'][ind]
  day_2=df_4['date_2'][ind]

  if type(df_4['date_1'][ind]) and type(df_4['date_2'][ind]) == datetime:
    if df_4['date_1'][ind] > df_4['date_2'][ind]:
      df_4['min_date_review'][ind]=df_4['date_1'][ind]
    else:
      df_4['min_date_review'][ind]=df_4['date_2'][ind]
  else:
    df_4['min_date_review'][ind]=='undefined'

df_4.head(1)

# Определим дату ближайшего отзыва

def get_prescription(d1,d2):
    if type(d1) and type(d2) == datetime:
        if d2 > d1:
            max_date = d2
        else:
            max_date = d1
        
        return (datetime.now() - max_date).days
    else:
        return 0

df_4['prescription'] = df_4.apply(lambda x: get_prescription(x['date_1'],x['date_2']),axis=1)

df_4.head(1)

"""**Когда был оставлен самый свежий отзыв? Введите ответ в формате yyyy-mm-dd.**"""

min_date = df_4.groupby(['prescription'])['min_date_review'].min().head(2)
print('ответ 4.4.1:', min_date)

"""# Ответ на задание 4.4.2:

**Какое максимальное количество дней отделяет даты публикации отзывов, размещённых на сайте ресторана? Введите количество дней в виде целого числа.**
"""

df_4.head(1)

### Количество дней между отзывами

def get_difference(d1,d2):
    if type(d1) and type(d2) == datetime:
        return (d1-d2).days
    else:
        return 0

df_4['distance_between_reviews'] = df_4.apply(lambda x: get_difference(x['date_1'],x['date_2']),axis=1)

# сразу посмотрим

df_4['distance_between_reviews'].head(5)

"""**Какое максимальное количество дней отделяет даты публикации отзывов, размещённых на сайте ресторана? Введите количество дней в виде целого числа.**"""

print('ответ 4.4.2 (id <кол-во дней>): \n', df_4.distance_between_reviews.sort_values(ascending=False).head(1))

"""# Дополнительная работа с признаками:

**Ranking и Number of Reviews**
"""

df_4.info()

"""**Признак Ranking**"""

df_4.Ranking.isnull().sum()

sns.heatmap(df_4.corr(), annot=True, cmap='ocean')
fig = plt.gcf()
fig.set_size_inches(15, 10)

"""**Признак Number of Reviews**"""

df_4['Number of Reviews'].isnull().sum()



"""# Ответ на задание 4.5:

**Какое значение метрики MAE вам удалось получить на этом этапе?**
"""

# Разбиваем датафрейм на части, необходимые для обучения и тестирования модели  
# Х - данные с информацией о ресторанах, у - целевая переменная (рейтинги ресторанов)  
df_model=df_4.copy(deep=True)

y = df_model['Rating'] 
X = df_model.drop(['Restaurant_id', 'Rating'], axis = 1)  
#y = df_model['Rating']  

X.info()
print('Размер вектора у=', len(y))

# Наборы данных с меткой "train" будут использоваться для обучения модели, "test" - для тестирования.  
# Для тестирования мы будем использовать 25% от исходного датасета.  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

"""**СОЗДАНИЕ, ОБУЧЕНИЕ И ТЕСТИРОВАНИЕ МОДЕЛИ**
(повторно, после очистки данных)

Сам процесс создания и тестирования модели занимает всего четыре строчки кода (при условии, что мы используем параметры по умолчанию):
"""

# Создаём модель  
regr = RandomForestRegressor(n_estimators=100)  
      
# Обучаем модель на тестовом наборе данных  
regr.fit(X_train, y_train)  
      
# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.  
# Предсказанные значения записываем в переменную y_pred  
y_pred = regr.predict(X_test)

X.info()

# удалим столбцы, содержащие данные типа object  

X = X.drop(['City', 'Cuisine Style', 'Price Range', 'Reviews', 'URL_TA', 'ID_TA', 'NAN_reviews',
            'reviews_fb_1', 'reviews_fb_2', 'date_1', 'date_2', 'min_date_review'], axis = 1)  

X.info()

# заполним пропущенные значения (None или NaN) 
# каким-то одним значением (нулём или средним арифметическим)
# для всего столбца

X = X.fillna(value=0)
X.info()

sns.heatmap(X.corr(), annot=True, cmap='ocean')
fig = plt.gcf()
fig.set_size_inches(15, 10)

# ранг ресторана на основании количества отзывов

X['Ranking_by_reviews'] = X['Ranking'] * X['Number of Reviews']

X.info()

sns.heatmap(X.corr(), annot=True, cmap='ocean')
fig = plt.gcf()
fig.set_size_inches(15, 10)

# Загружаем специальный инструмент для разбивки:  
from sklearn.model_selection import train_test_split  
      
# Наборы данных с меткой "train" будут использоваться для обучения модели, "test" - для тестирования.  
# Для тестирования мы будем использовать 25% от исходного датасета.  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

# Импортируем необходимые библиотеки:  
from sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели  
from sklearn import metrics # инструменты для оценки точности модели  
      
# Создаём модель  
regr = RandomForestRegressor(n_estimators=100)  
      
# Обучаем модель на тестовом наборе данных  
regr.fit(X_train, y_train)  
      
# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.  
# Предсказанные значения записываем в переменную y_pred  
y_pred = regr.predict(X_test) 
y_pred

"""**Ответ на задание 4.5:**

**Какое значение метрики MAE вам удалось получить на этом этапе?**

**КАК УЗНАТЬ, ЧТО МОЯ МОДЕЛЬ ХОРОШАЯ?**
"""

# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются  
# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.  
print('ответ на задание 4.5 - MAE:', round(metrics.mean_absolute_error(y_test, y_pred),2))

"""**Напишите список не менее чем из пяти признаков, которые вы добавили в датафрейм и дайте краткое пояснение, что означает каждый из признаков.**

1. исходный признак Reviews включает данные о текстовом комментарии посетителей ресторанов и данные о дате, когда был оставлен комментарий. Признак Reviews разделен на 4-е признака: два признака с текстовыми комментариями и два признака с датами

2. исходный признак Cusine Style очищен от лишних символов. Данные в исходном признаке предствалены в виде списка с вложенным списком. Поэтому выполнено разделение вложенного списка на отдельные элементы в пределах строки.

3. исходный признак Ranking и Number of Reviews объединены в новый признак Ranking by Reviews - что позволит оценить, насколько рейтинг ресторана зависит от количества отзывов о нем. Второй признак, который, возможно, оказывает влияние на вновь сформированный признак Ranking by Reviews, - давность оставленного отзыва (Prescription). Предварительный анализ по тепловой карте показывает, что есть взаимосвязь. 

4. ________________
5. ________________
6. ________________

# 5.И снова о чайниках
 
ОРГАНИЗАЦИОННАЯ ИНФОРМАЦИЯ

Что нужно сделать на этом шаге?
Добавить в датафрейм несколько столбцов, содержащих данные особого типа — dummy variables, или фиктивные переменные.
Подобрать такую комбинацию столбцов, которая повысит точность прогноза, сделанного моделью.

**Под категориальным значением** мы будем понимать признак, значения которого означают принадлежность объекта к одной или нескольким категориям. Например, марка автомобиля (одна категория), цвет или цвета (одна или несколько категорий).

*Начинающие исследователи данных при работе с такими признаками часто совершают ошибку, просто заменяя определённые строковые значения признака числовыми показателями. Например, присваивают красному цвету код «1», зелёному — «2», жёлтому — «3» и т.д. Такой подход неверен, так как он предполагает, что объекты красного цвета обладают меньшим значением признака «цвет» по сравнению с зелёными и особенно — жёлтыми, что неверно, по сути.*

*Вторая проблема возникает в ситуации, когда в объекте встречается несколько цветов, например, красный и жёлтый. Какое число может выразить цвет такого объекта? Среднее арифметическое между «красным» и «жёлтым»? То есть объекты, в которых присутствуют красный и жёлтый цвета, на самом деле зелёные? :) Конечно же, это не так.*

**Для перевода категориальных признаков в числовые в машинном обучении довольно часто используется концепция, называемая dummy variables. Суть концепции заключается в том, что для каждой категории признака (в нашем примере — для каждого цвета) создаётся отдельная переменная, которая может быть равна единице, если объект содержит данный цвет и нулю — если нет.**
"""

df_model=df_4.copy(deep=True)

df_model.info() # датасет использованной для тестирования модели на второй попытке

y3 = df_model['Rating'] 
X3 = df_model.drop(['Restaurant_id', 'Rating'], axis = 1)  
#y = df_model['Rating'] 

X3.info()

X3['Ranking_by_reviews'] = X3['Ranking'] * X3['Number of Reviews']
X3.info()

"""# Вводим Dummy Variables
для признаков:

- City (город, страна)
- Price Range (высокий, средний, низкий (бюджетный))
- Reviews (для новых признаков - reviews_fb_1, reviews_fb_2) (хороший, плохой)


"""

# удалим столбцы, содержащие данные типа object  

X3 = X3.drop(['Cuisine Style', 'Reviews', 'URL_TA', 'ID_TA', 'NAN_reviews',
              'date_1', 'date_2', 'min_date_review'], axis = 1)  

X3.info()

# заполним пропущенные значения (None или NaN) 
# каким-то одним значением (нулём или средним арифметическим)
# для всего столбца

X3 = X3.fillna(value=0)
X3.info()

df_5=X3.copy(deep=True)

"""# АНАЛИЗИРУЕМ оставшиеся признаки 
# для НОВОЙ МОДЕЛИ
"""

# признак City

df_5.City.value_counts()

# Определим признак Country
# который будет отражать популярность ресторанов 
# для посещения населением (местные жители и туристы; 
# сделать различие между ними по данному датасету не представляется возможным)

# c Kaggle из BaseLine
# берем датасет, включающий пару "город"-"страна"


# принадлежность к стране
city_of_country = {
    'London': 'United Kingdom',
    'Paris': 'France',
    'Madrid': 'Spain',
    'Barcelona': 'Spain',
    'Berlin': 'Germany',
    'Milan': 'Italy',
    'Rome': 'Italy',
    'Prague': 'Czech Republic',
    'Lisbon': 'Portugal',
    'Vienna': 'Austria',
    'Amsterdam': 'Netherlands',
    'Brussels': 'Belgium',
    'Hamburg': 'Germany',
    'Munich': 'Germany',
    'Lyon': 'France',
    'Stockholm': 'Sweden',
    'Budapest': 'Hungary',
    'Warsaw': 'Poland',
    'Dublin': 'Ireland',
    'Copenhagen': 'Denmark',
    'Athens': 'Greece',
    'Edinburgh': 'United Kingdom',
    'Zurich': 'Switzerland',
    'Oporto': 'Portugal',
    'Geneva': 'Switzerland',
    'Krakow': 'Poland',
    'Oslo': 'Norway',
    'Helsinki': 'Finland',
    'Bratislava': 'Slovak Republic',
    'Luxembourg': 'Luxembourg',
    'Ljubljana': 'Slovenia'
}

city_of_country

# сделаем дополнительную разметку в датасете с городами-странами
# добавим dummy variables - "Принадлежность к города к стране":

country_of_country = {
    'United Kingdom': 1,
    'France': 2,
    'Spain': 3,
    'Germany': 4,
    'Italy': 5,
    'Czech Republic': 6,
    'Portugal': 7,
    'Austria': 8,
    'Netherlands': 9,
    'Belgium': 10,
    'Sweden': 11,
    'Hungary': 12,
    'Poland': 13,
    'Ireland': 14,
    'Denmark': 15,
    'Greece': 16,
    'Switzerland': 17,
    'Norway': 18,
    'Finland': 19,
    'Slovak Republic': 20,
    'Luxembourg': 21,
    'Slovenia': 22
}



country_of_country

# создаём колонку со странами

df_5['Country'] = df_5['City'].map(city_of_country)

df_5.info()

df_5.Country.value_counts(ascending=False).head(22)

# создаём колонку с признаком "Популярность стран по городам"

df_5['Country_popularity'] = df_5['Country'].map(country_of_country)

df_5.info()

# Рейтинг стран 
# (по количеству городов, в которых есть рестораны, в которых были посетители)

df_5.groupby(['Country_popularity','Country'])['City'].count().sort_values(ascending=False)

sns.heatmap(df_5.corr(), annot=True, cmap='ocean')
fig = plt.gcf()
fig.set_size_inches(15, 10)

# признак Price Range

df_5[df_5['Price Range']=='$$$$'].value_counts()

#groupby(['Country_popularity','Country'])['City'].count().sort_values(ascending=False)
df_5.groupby(['Country','City'])['Price Range'].describe()

# создаем новый признак 
# на основе Price Range
# учитываем, что есть четыре диапазона оценки
# "0" - низкий уровень или неопределен статус ресторана (без звезд) = 0
# "$" - уровень 1 (одна звезда) = 1
# "$$ - $$$" - уровень 2-3 (две-три звезды) = 2
# "$$$$" - уровень 4 (четыре звезды) = 4

price_range_dict = {'0': 0, '$': 1, '$$ - $$$': 2, '$$$$': 3}
df_5['Price_Rating'] = df_5['Price Range'].map(price_range_dict)

df_5.info()

df_5['Price_Rating'].isna().sum()

df_5['Price_Rating'].sort_values(ascending=True)

# заполняем пропуски
# формируем группу "0"

df_5['NAN_price_range'] = pd.isna(df_5['Price_Rating']).astype('uint8')
df_5['Price_Rating'].fillna(df_5['Price_Rating'].mean(), inplace=True)

# на Kaggle этот код отсутствует, т.к. сделано заполнение значением "0"
# для всех ячеек с Nan

df_5['Price_Rating'].head()

sns.heatmap(df_5.corr(), annot=True, cmap='ocean')
fig = plt.gcf()
fig.set_size_inches(15, 10)

df_5.info()

# признак reviews_fb_1 и reviews_fb_2

df_5.reviews_fb_1.value_counts().head(5)

# словарь слов - хорошие комментарии
# расширен словарь хороших комментариев

good_reviews = [
    'very',
    'good',
    'nice',
    'amazing',
    'great',
    'best',
    'loved',
    'exellent',
    'a.m.a.z.i.n.g',
    'lovely',
    'love',
    'top',
    'fast',
    'super',
    'delicious',
    'beautiful',
    'fabulous',
    'wonderful',
    'brilliant',
    'not bad',
    'enjoyable',
    'fantastic',
    'yummy',
    'perfect',
    'tasty',
    'awesome',
    'hidden gem',
    'just ok',
    'outstanding',
    'wow',
    'friendly',
    'welcoming',
    'ideal'
            ]

good_reviews

df_5.reviews_fb_2.value_counts().head(5)

# словарь слов - плохие комментарии
# расширен словарь плохих комментариев

bad_reviews = [
    'slow',
    'bad', 
    'small',
    'missed',
    'low',
    'busy',
    'nightmare',
    'overrated',
    'worst',
    'nothing special',
    'hidden gem',
    'average',
    'disappointing',
    'below average'
            ]

bad_reviews

# определение хороших и плохих отзывов
# (функция взята из baseline на Kaggle)

def get_reviews(row, reviews, good_bad):
    
    # строка дс, список слов, признак отзыва
    
    if row != str:
        return 0
    for rev in reviews:
        if rev.lower() in row.lower():
            return (good_bad)
        else:
            return 0

# считаем хорошие комменты

df_5['good_reviews'] = df_5['reviews_fb_1'].apply(lambda x: get_reviews(x, good_reviews, 2))
df_5['good_reviews'] = df_5['reviews_fb_2'].apply(lambda x: get_reviews(x, good_reviews, 2))

df_5.good_reviews.describe()

# считаем плохие комменты

df_5['bad_reviews'] = df_5['reviews_fb_1'].apply(lambda x: get_reviews(x, bad_reviews, 1))
df_5['bad_reviews'] = df_5['reviews_fb_2'].apply(lambda x: get_reviews(x, bad_reviews, 1))

df_5.bad_reviews.describe()

df_5.info()

"""**Новые признаки good_reviews и bad_reviews**
оказались пустыми
как их исправить и заполнить значением признака отзыва (хороший/плохой) - пока не знаю.
"""

sns.heatmap(df_5.corr(), annot=True, cmap='ocean')
fig = plt.gcf()
fig.set_size_inches(15, 10)

"""**Дорабатываем города: создаем для них отдельные столбцы**
для того, чтобы ввести значения dummy variables - "0" и "1"
"""

# закодируем данные о городах и странах

le = LabelEncoder()

le.fit(df_5['Country'])
df_5['country_cod'] = le.transform(df_5['Country'])

le.fit(df_5['City'])
df_5['city_cod'] = le.transform(df_5['City'])

# разобъём города дополнительно на колонки

df_5 = pd.get_dummies(df_5, columns=['City',], dummy_na=True)

df_5.info()

# Посмотрим распределение признака

sns.heatmap(df_5.corr(), annot=True, cmap='ocean')
fig = plt.gcf()
fig.set_size_inches(15, 15)

df_train=df_5

# Посмотрим распределение признака

plt.rcParams['figure.figsize'] = (10,7)
df_train['Ranking'].hist(bins=100)

# целевая переменная
# y_train

# Посмотрим распределение целевой переменной

y_train.value_counts(ascending=True).plot(kind='barh')

df_6=df_5.copy(deep=True)

"""# ВЫВОД по новым параметрам 

# DUMMY VARIABLES (введены для признаков City, Price range, Reviews (для этого признака неудачно))

- bad_reviews - определяют количество плохих комментариев (не удалось корректно посчитать)
- good_reviews - определяют количество хороших комментариев (не удалось корректно посчитать)
- NAN_price_range - 
- Price_Rating - оценивает ценовые категории, к которым относится ресторан; информативный параметр
- Country_popularity - данный параметр оценивает пополуярность страны в целом. Параметр обобщенный, т.к. он зависит от покупательской способности, от бюджетности стоимости, от доступности и привлекательности ресторанов и т.п.
- Ranking_by_reviews - параметр определяющий рейтинг ресторанов по комментарию.

# Расчет по НОВОЙ МОДЕЛИ (третья модель)
"""

X_6 = df_6.drop(['Price Range', 'reviews_fb_1', 'reviews_fb_2','Country'], axis = 1)  

X_6.info()

# Разбиваем датафрейм на части, необходимые для обучения и тестирования модели  
# Х - данные с информацией о ресторанах, у - целевая переменная (рейтинги ресторанов)  
df_model=X_6.copy(deep=True)

y = df_4['Rating'] 
X = df_model 

X.info()
print('Размер вектора у=', len(y))

# Наборы данных с меткой "train" будут использоваться для обучения модели, "test" - для тестирования.  
# Для тестирования мы будем использовать 25% от исходного датасета.  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

# Создаём модель  
regr = RandomForestRegressor(n_estimators=100)  
      
# Обучаем модель на тестовом наборе данных  
regr.fit(X_train, y_train)  
      
# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.  
# Предсказанные значения записываем в переменную y_pred  
y_pred = regr.predict(X_test)

sns.heatmap(X.corr(), annot=True, cmap='ocean')
fig = plt.gcf()
fig.set_size_inches(15, 10)

# Загружаем специальный инструмент для разбивки:  
from sklearn.model_selection import train_test_split  
      
# Наборы данных с меткой "train" будут использоваться для обучения модели, "test" - для тестирования.  
# Для тестирования мы будем использовать 25% от исходного датасета.  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

# Импортируем необходимые библиотеки:  
from sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели  
from sklearn import metrics # инструменты для оценки точности модели  
      
# Создаём модель  
regr = RandomForestRegressor(n_estimators=100)  
      
# Обучаем модель на тестовом наборе данных  
regr.fit(X_train, y_train)  
      
# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.  
# Предсказанные значения записываем в переменную y_pred  
y_pred = regr.predict(X_test) 
y_pred

"""**Ответ на задание 5.1:**

**Какое значение метрики MAE вам удалось получить на этом этапе?**
"""

# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются  
# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.  
print('ответ на задание 5.1 - MAE:', round(metrics.mean_absolute_error(y_test, y_pred),2))

"""# 6.Kaggle. Начало

ОРГАНИЗАЦИОННАЯ ИНФОРМАЦИЯ

Что нужно сделать на этом шаге?
- узнать, что такое Kaggle и зачем он нужен;
- зарегистрироваться на Kaggle.com (желательно под таким же ником, как и в Слаке);
- разобраться, как работать с kaggle notebooks;
- отправить свой первый submit.

В этом соревновании вам будет предложен датасет, содержащий сведения о ресторанах. С помощью имеющего в вашем распоряжении кода, вам необходимо создать модель, использующую алгоритм RandomForestRegression, которая будет прогнозировать рейтинг ресторана по версии TripAdvidor.

Для победы в конкурсе вам необходимо качественно очистить датасет, подобрать подходящие значения для заполнения пропусков и создать новые признаки на основе той информации, которую вы сможете извлечь из имеющихся в вашем распоряжении данных.

Условия соревнования:
Все участники должны использовать один и тот же алгоритм с параметрами, заданными по умолчанию.
Разрешено использовать внешние данные.
Решения буду проверяться преподавателями на их адекватность и воспроизводимость.
Во вкладке Notebooks этого соревнования для Вас доступно Базовое решение (Baseline)

https://www.kaggle.com/c/sf-dst-restaurant-rating
"""



