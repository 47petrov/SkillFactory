{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FIT-M_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fg7pzJ1dtUX"
      },
      "source": [
        "# РЕШЕНИЕ ЗАДАЧИ \"ТРАССА: оптимальная траектория\"\n",
        "\n",
        "Требуется определить оптимальную траекторию при движении гоночной машины по трассе. \n",
        "\n",
        "Параметры управления: угол ориентации машины и её скорость\n",
        "Параметры системы: координаты x и y\n",
        "Выходной параметр: время движения по трассе (т.е. скорость должна быть как можно выше)\n",
        "Критерий оптимизации: минимальное расстояние между предложенной трассой и расчетной трассой\n",
        "\n",
        "Трасса определяется координатами x и y.\n",
        "\n",
        "Описание задачи доступно по ссылке: https://drive.google.com/file/d/13k3FiNV7XmCUuIKt-00d8ua5r5QVYpBK/view?usp=sharing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4J0bF8Ndrn1"
      },
      "source": [
        "**Загрузка библиотек**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-4NVHKcgtq_"
      },
      "source": [
        "# УСТАНОВКА БИБЛИОТЕК\n",
        "\n",
        "!pip install -q keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "Td8IOAzbkcwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# установка tf-Agent как в руководстве\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet"
      ],
      "metadata": {
        "id": "UuHmHY8_uhYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xvfbwrapper"
      ],
      "metadata": {
        "id": "wlqswHmg1bRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gym[all]\n",
        "#!pip install gym[box2d]"
      ],
      "metadata": {
        "id": "vLUQwqIU9e3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install mujoco-py==0.5.7"
      ],
      "metadata": {
        "id": "OvPXbT2P6Qiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install required system dependencies\n",
        "#!apt-get install -y xvfb x11-utils\n",
        "\n",
        "## install required python dependencies (might need to install additional gym extras depending)\n",
        "#!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "##import\n",
        "#import pyvirtualdisplay\n",
        "#_display = pyvirtualdisplay.Display(visible=0,  # remember to use visible=0 and not False\n",
        "                                    size=(1400, 900))\n",
        "#_ = _display.start()\n",
        "\n",
        "##check\n",
        "#!echo $DISPLAY"
      ],
      "metadata": {
        "id": "pOrroJ6N6mT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFPWrLLVdq1b"
      },
      "source": [
        "# ИМПОРТ БИБЛИОТЕК\n",
        "\n",
        "import os\n",
        "\n",
        "#numpy & keras\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "# для визуализации данных\n",
        "import cv2 as cv\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "%matplotlib inline\n",
        "\n",
        "# библиотека для обработки даты, времени\n",
        "import datetime as dt\n",
        "import time\n",
        "from datetime import datetime, date, time, timedelta\n",
        "\n",
        "#\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#tensorflow & keras\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "3jhuJG9UvSd1",
        "outputId": "8ef1ddeb-c2a9-4308-af8a-fb94417e3ea5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (np.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzT5-M9U4QNm",
        "outputId": "2d2066ab-1399-4ab4-94fc-6386ac231ee6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.19.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# из Notebook @author: Vadim Created on Thu Dec 16 15:50:11 2021\n",
        "import argparse #argparse - это модуль для обработки аргументов командной строки\n",
        "import pickle #pickle - Модуль pickle предоставляет функции и классы для сериализации и десериализации объектов\n",
        "\n",
        "# \n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "import tf_agents.trajectories.time_step as ts\n",
        "\n",
        "from tf_agents.policies.policy_saver import PolicySaver\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "import re"
      ],
      "metadata": {
        "id": "au8ldV3Bv03U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xvfbwrapper import Xvfb\n",
        "\n",
        "vdisplay = Xvfb()\n",
        "vdisplay.start()\n",
        "\n",
        "# launch stuff inside\n",
        "# virtual display here.\n",
        "\n",
        "vdisplay.stop()"
      ],
      "metadata": {
        "id": "80OxkcBI1l1_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# КЛАСС из ноутбука Notebook\n",
        "@author: Vadim"
      ],
      "metadata": {
        "id": "esEPEOWXwfWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOTrainer:\n",
        "    \n",
        "    def __init__(self, ppo_agents, train_env, eval_env, size=(96, 96),\n",
        "                 normalize=True, num_frames=1, num_channels=3,\n",
        "                 use_tensorboard=True, add_to_video=True,\n",
        "                 use_separate_agents=False, use_self_play=False,\n",
        "                 num_agents=2, use_lstm=False, experiment_name=\"\",\n",
        "                 collect_steps_per_episode=1000, total_epochs=1000,\n",
        "                 total_steps=1e6, eval_steps_per_episode=1000,\n",
        "                 eval_interval=100, num_eval_episodes=5, epsilon=0.0,\n",
        "                 save_interval=500, log_interval=1):\n",
        "\n",
        "        self.train_env = train_env\n",
        "        self.eval_env = eval_env  \n",
        "\n",
        "        self.size = size\n",
        "        self.H, self.W = self.size[0], self.size[1]  \n",
        "        self.normalize = normalize\n",
        "        self.num_frames = num_frames\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        self.use_separate_agents = use_separate_agents  \n",
        "        self.use_self_play = use_self_play  \n",
        "        self.use_lstm = use_lstm  \n",
        "        self.num_agents = num_agents\n",
        "\n",
        "        self.max_buffer_size = collect_steps_per_episode \n",
        "        self.collect_steps_per_episode = collect_steps_per_episode  \n",
        "        self.epochs = total_epochs  \n",
        "        self.total_steps = total_steps  \n",
        "        self.global_step = 0  \n",
        "        self.epsilon = epsilon \n",
        "\n",
        "        print(\"Total steps: {}\".format(self.total_steps))\n",
        "\n",
        "        # Create N different PPO agents\n",
        "        if use_separate_agents and self.num_agents > 1:\n",
        "            self.agents = ppo_agents  \n",
        "            for agent in self.agents:\n",
        "                agent.initialize()  \n",
        "            self.actor_nets = [self.agents[i]._actor_net \\\n",
        "                               for i in range(self.num_agents)]\n",
        "            self.value_nets = [self.agents[i]._value_net \\\n",
        "                               for i in range(self.num_agents)]\n",
        "            self.eval_policies = [self.agents[i].policy \\\n",
        "                                  for i in range(self.num_agents)]\n",
        "            self.collect_policies = [self.agents[i].collect_policy \\\n",
        "                                     for i in range(self.num_agents)]\n",
        "            self.replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "                self.agents[0].collect_data_spec,\n",
        "                batch_size=self.train_env.batch_size,\n",
        "                max_length=self.max_buffer_size)  # Create shared replay buffer\n",
        "\n",
        "        else:\n",
        "            self.agent = ppo_agents\n",
        "            self.actor_net = self.agent._actor_net\n",
        "            self.value_net = self.agent._value_net\n",
        "            self.eval_policy = self.agent.policy\n",
        "            self.collect_policy = self.agent.collect_policy\n",
        "            self.replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "                self.agent.collect_data_spec,\n",
        "                batch_size=self.train_env.batch_size,\n",
        "                max_length=self.max_buffer_size)\n",
        "\n",
        "        if self.num_agents > 1:  \n",
        "            self.observation_wrappers = \\\n",
        "                            [ObservationWrapper(size=self.size, normalize=self.normalize,\n",
        "                                                num_channels=self.num_channels,\n",
        "                                                num_frames=self.num_frames)\n",
        "                             for i in range(self.num_agents)]\n",
        "\n",
        "        else:  # Single observation wrapper for single car\n",
        "            self.observation_wrapper = ObservationWrapper(size=self.size,\n",
        "                                                          normalize=self.normalize,\n",
        "                                                          num_channels=self.num_channels,\n",
        "                                                          num_frames=self.num_frames)\n",
        "\n",
        "        # Evaluation\n",
        "        self.num_eval_episodes = num_eval_episodes  \n",
        "\n",
        "        if self.use_separate_agents:\n",
        "            self.eval_returns = [[] for i in range(self.num_agents)]\n",
        "\n",
        "        else:\n",
        "            self.eval_returns = []\n",
        "\n",
        "        self.eval_interval = eval_interval  \n",
        "        self.max_eval_episode_steps = eval_steps_per_episode  \n",
        "        self.time_ext = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "        self.log_interval = log_interval\n",
        "        self.video_train = []\n",
        "        self.video_eval = []\n",
        "        self.add_to_video = add_to_video\n",
        "        self.FPS = 50  \n",
        "        self.policy_save_dir = os.path.join(os.path.split(__file__)[0], \"models\",\n",
        "                                            experiment_name.format(self.time_ext))\n",
        "        self.save_interval = save_interval\n",
        "        if not os.path.exists(self.policy_save_dir):\n",
        "            print(\"Directory {} does not exist;\"\n",
        "                  \" creating it now\".format(self.policy_save_dir))\n",
        "            os.makedirs(self.policy_save_dir, exist_ok=True)\n",
        "\n",
        "        if self.use_separate_agents:\n",
        "            self.train_savers = [PolicySaver(self.collect_policies[i],\n",
        "                                             batch_size=None) for i in\n",
        "                                 range(self.num_agents)]\n",
        "            self.eval_savers = [PolicySaver(self.eval_policies[i],\n",
        "                                            batch_size=None) for i in\n",
        "                                range(self.num_agents)]\n",
        "\n",
        "        else:\n",
        "            self.train_saver = PolicySaver(self.collect_policy, batch_size=None)\n",
        "            self.eval_saver = PolicySaver(self.eval_policy, batch_size=None)\n",
        "\n",
        "        self.log_dir = os.path.join(os.path.split(__file__)[0], \"logging\",\n",
        "                                    experiment_name.format(self.time_ext))\n",
        "        self.tb_file_writer = tf.summary.create_file_writer(self.log_dir)\n",
        "        if not os.path.exists(self.log_dir):\n",
        "            os.makedirs(self.log_dir, exist_ok=True)\n",
        "        self.use_tensorboard = use_tensorboard  \n",
        "        self.size = size\n",
        "        self.H, self.W = self.size\n",
        "        self.stacked_channels = self.num_channels * self.num_frames\n",
        "        if self.use_tensorboard:\n",
        "            self.tb_gif_train = np.zeros((self.collect_steps_per_episode,\n",
        "                                         self.num_agents, self.H, self.W,\n",
        "                                          self.stacked_channels))\n",
        "            self.tb_gif_eval = np.zeros((self.max_eval_episode_steps,\n",
        "                                        self.num_agents, self.H, self.W,\n",
        "                                         self.stacked_channels))\n",
        "        # Devices\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        num_gpus = len([x.name for x in local_device_protos if\n",
        "                        x.device_type == 'GPU'])\n",
        "        self.use_gpu = num_gpus > 0\n",
        "\n",
        "    def is_last(self, mode='train'):\n",
        "        \n",
        "        if mode == 'train':\n",
        "            step_types = self.train_env.current_time_step().step_type.numpy()\n",
        "        elif mode == 'eval':\n",
        "            step_types = self.eval_env.current_time_step().step_type.numpy()\n",
        "\n",
        "\n",
        "        is_last = bool(min(np.count_nonzero(step_types == 2), 1))\n",
        "        return is_last\n",
        "\n",
        "    def get_agent_timesteps(self, time_step, step=0,\n",
        "                            only_ego_car=False, ego_car_index=0, max_steps=1000):\n",
        "        \n",
        "        discount = time_step.discount\n",
        "        if len(discount.numpy().shape) > 1 or discount.numpy().shape[0] > 1:\n",
        "            discount = discount[0]\n",
        "        discount = tf.convert_to_tensor(discount, dtype=tf.float32,\n",
        "                                        name='discount')\n",
        "\n",
        "        \n",
        "        if step == 0:  \n",
        "            step_type = 0\n",
        "        elif step == max_steps-1:  \n",
        "            step_type = 2\n",
        "        else:  # Middle time step\n",
        "            step_type = 1\n",
        "        step_type = tf.convert_to_tensor([step_type], dtype=tf.int32,\n",
        "                                         name='step_type')\n",
        "\n",
        "        # Extract rewards for all agents\n",
        "        try:\n",
        "            R = [tf.convert_to_tensor(time_step.reward[:, car_id],\n",
        "                                      dtype=tf.float32, name='reward') \\\n",
        "                 for car_id in range(self.num_agents)]\n",
        "        except:\n",
        "            R = [tf.convert_to_tensor(time_step.reward,\n",
        "                                      dtype=tf.float32, name='reward') \\\n",
        "                 for _ in range(self.num_agents)]\n",
        "\n",
        "        if only_ego_car:\n",
        "            processed_observation = \\\n",
        "                self._process_observations(time_step, only_ego_car=only_ego_car,\n",
        "                                           ego_car_index=ego_car_index)\n",
        "            return ts.TimeStep(step_type, R[ego_car_index], discount,\n",
        "                                   tf.convert_to_tensor(processed_observation,\n",
        "                                   dtype=tf.float32, name='observations'))\n",
        "\n",
        "        else:\n",
        "            processed_observations = \\\n",
        "                self._process_observations(time_step, only_ego_car=only_ego_car,\n",
        "                                           ego_car_index=ego_car_index)\n",
        "            return [ts.TimeStep(step_type, R[car_id], discount,\n",
        "                    tf.convert_to_tensor(processed_observations[car_id],\n",
        "                                         dtype=tf.float32, name='observations'))\n",
        "                    for car_id in range(self.num_agents)]\n",
        "\n",
        "    def _process_observations(self, time_step, only_ego_car=False,\n",
        "                              ego_car_index=0):\n",
        "\n",
        "        if only_ego_car:\n",
        "            input_observation = time_step.observation[:, ego_car_index]\n",
        "\n",
        "            if self.num_agents > 1:  \n",
        "                wrapper = self.observation_wrappers[ego_car_index]\n",
        "            else:\n",
        "                wrapper = self.observation_wrapper\n",
        "\n",
        "            processed_observation = wrapper.get_obs_and_step(input_observation)\n",
        "\n",
        "            return tf.convert_to_tensor(processed_observation, dtype=tf.float32,\n",
        "                                        name='observations')\n",
        "\n",
        "        else:\n",
        "            input_observations = [time_step.observation[:, car_id] for\n",
        "                                  car_id in range(self.num_agents)]\n",
        "            if self.num_agents > 1:  \n",
        "                processed_observations = \\\n",
        "                    [wrapper.get_obs_and_step(input_observation)\n",
        "                     for wrapper, input_observation in\n",
        "                     zip(self.observation_wrappers, input_observations)]\n",
        "\n",
        "            else:  # Single car\n",
        "                processed_observations = \\\n",
        "                    [self.observation_wrapper.get_obs_and_step(\n",
        "                        input_observations[0])]\n",
        "\n",
        "            return [tf.convert_to_tensor(processed_observations[car_id],\n",
        "                                    dtype=tf.float32, name='observations')\n",
        "                    for car_id in range(self.num_agents)]\n",
        "\n",
        "    def collect_step(self, step=0, ego_car_index=0, use_greedy=False,\n",
        "                     add_to_video=False):\n",
        "        \n",
        "        time_step = self.train_env.current_time_step()\n",
        "\n",
        "        actions = []\n",
        "\n",
        "        agent_timesteps = self.get_agent_timesteps(time_step, step=step,\n",
        "                                                   only_ego_car=False,\n",
        "                                                   max_steps=self.max_eval_episode_steps-1)\n",
        "\n",
        "        if self.use_separate_agents:\n",
        "            ego_agent_policy = self.collect_policies[ego_car_index]\n",
        "        else:\n",
        "            ego_agent_policy = self.collect_policy\n",
        "\n",
        "\n",
        "        NUM_AGENTS = 1 \n",
        "        for car_id in range(NUM_AGENTS):\n",
        "\n",
        "            if car_id == ego_car_index:\n",
        "                ego_agent_ts = agent_timesteps[car_id]\n",
        "                ego_action_step = ego_agent_policy.action(ego_agent_ts)\n",
        "                if use_greedy:\n",
        "                    actions.append(ego_action_step.info['loc'])  \n",
        "                else:\n",
        "                    actions.append(ego_action_step.action)  \n",
        "\n",
        "                if self.add_to_video:\n",
        "                    rendered_state = time_step.observation[:, car_id].numpy()\n",
        "                    if self.stacked_channels > 3: \n",
        "                        rendered_state = rendered_state[:, :, :, :3] \n",
        "                    self.video_train.append(rendered_state)\n",
        "\n",
        "            elif self.use_separate_agents:\n",
        "                other_agent_ts = agent_timesteps[car_id]\n",
        "                action_step = self.eval_policies[car_id].action(other_agent_ts)\n",
        "                actions.append(action_step.action)\n",
        "\n",
        "            elif self.use_self_play:\n",
        "                other_agent_ts = agent_timesteps[car_id]\n",
        "                action_step = self.eval_policy.action(other_agent_ts)\n",
        "                actions.append(action_step.action)\n",
        "\n",
        "        if self.use_tensorboard:\n",
        "            processed_observations = self._process_observations(time_step,\n",
        "                                                               only_ego_car=False)\n",
        "            self.tb_gif_train[step] = tf.convert_to_tensor(processed_observations)\n",
        "\n",
        "        action_tensor = tf.convert_to_tensor([tf.stack(tuple(actions), axis=1)])\n",
        "\n",
        "        next_time_step = self.train_env.step(action_tensor)\n",
        "        ego_agent_next_ts = self.get_agent_timesteps(next_time_step, step=step+1,\n",
        "                                                     ego_car_index=ego_car_index,\n",
        "                                                     only_ego_car=True,\n",
        "                                                     max_steps=self.collect_steps_per_episode-1)\n",
        "\n",
        "        traj = trajectory.from_transition(ego_agent_ts, ego_action_step,\n",
        "                                          ego_agent_next_ts)\n",
        "        self.replay_buffer.add_batch(traj)\n",
        "\n",
        "        if add_to_video:\n",
        "            rendered_state = time_step.observation[:, ego_car_index].numpy()\n",
        "            if self.num_frames > 1:  \n",
        "                rendered_state = rendered_state[:, :, :, :3]  # First frame\n",
        "            self.video_train.append(rendered_state)\n",
        "\n",
        "        return float(ego_agent_ts.reward)\n",
        "\n",
        "    def collect_episode(self, epoch=0, ego_car_index=0, add_to_video=False):\n",
        "        \n",
        "        episode_reward = 0  \n",
        "        step = 0  \n",
        "\n",
        "        self.train_env.reset()\n",
        "\n",
        "        use_greedy = float(np.random.binomial(n=1, p=self.epsilon))\n",
        "\n",
        "        while step < self.collect_steps_per_episode and \\\n",
        "                not self.is_last(mode='train'):\n",
        "            episode_reward += self.collect_step(add_to_video=add_to_video,\n",
        "                                                step=step, use_greedy=use_greedy,\n",
        "                                                ego_car_index=ego_car_index)\n",
        "            step += 1\n",
        "\n",
        "        self.global_step += step\n",
        "\n",
        "        if self.use_tensorboard:\n",
        "            with self.tb_file_writer.as_default():\n",
        "                tf.summary.scalar(\"Average Training Reward\", float(episode_reward),\n",
        "                                  step=self.global_step)\n",
        "                frames = self.tb_gif_train\n",
        "                video_summary(\"train/grid\", frames, fps=self.FPS,\n",
        "                              step=self.global_step, channels=self.num_channels)\n",
        "\n",
        "            self.tb_gif_train = np.zeros((self.collect_steps_per_episode,\n",
        "                                         self.num_agents, self.H, self.W,\n",
        "                                          self.stacked_channels))\n",
        "\n",
        "    def compute_average_reward(self, ego_car_index=0):\n",
        "        \n",
        "        total_return = 0.0  \n",
        "\n",
        "        for e in range(self.num_eval_episodes):\n",
        "            time_step = self.eval_env.reset()\n",
        "\n",
        "            # Initialize step counter and episode_return\n",
        "            i = 0\n",
        "            episode_return = 0.0\n",
        "            while i < self.max_eval_episode_steps and \\\n",
        "                    not self.is_last(mode='eval'):\n",
        "                actions = []\n",
        "\n",
        "                agent_timesteps = self.get_agent_timesteps(time_step, step=i)\n",
        "                for car_id in range(self.num_agents):\n",
        "\n",
        "                    if car_id == ego_car_index:  \n",
        "                        ego_agent_ts = agent_timesteps[car_id]\n",
        "                        rendered_state = ego_agent_ts.observation.numpy()\n",
        "                        if self.num_frames > 1:  \n",
        "                            rendered_state = rendered_state[..., :3] \n",
        "                        self.video_eval.append(rendered_state)\n",
        "\n",
        "                        if self.use_separate_agents:  \n",
        "                            ego_action_step = self.eval_policies[car_id].action(ego_agent_ts)\n",
        "                            actions.append(ego_action_step.action)   \n",
        "\n",
        "                        elif self.use_self_play:  \n",
        "                            ego_action_step = self.eval_policy.action(ego_agent_ts)\n",
        "                            actions.append(ego_action_step.action) \n",
        "\n",
        "                    elif self.use_separate_agents:\n",
        "                        other_agent_ts = agent_timesteps[car_id]\n",
        "                        action_step = self.eval_policies[car_id].action(other_agent_ts)\n",
        "                        actions.append(action_step.action)\n",
        "\n",
        "                   \n",
        "                    elif self.use_self_play:\n",
        "                        other_agent_ts = agent_timesteps[car_id]\n",
        "                        action_step = self.eval_policy.action(other_agent_ts)\n",
        "                        actions.append(action_step.action)\n",
        "\n",
        "                action_tensor = tf.convert_to_tensor([tf.stack(tuple(actions),\n",
        "                                                               axis=1)])\n",
        "\n",
        "                time_step = self.eval_env.step(action_tensor)\n",
        "\n",
        "                if self.use_tensorboard:\n",
        "                    processed_observations = self._process_observations(time_step,\n",
        "                                                                        only_ego_car=False)\n",
        "                    self.tb_gif_eval[i] = tf.convert_to_tensor(processed_observations)\n",
        "\n",
        "                episode_return += ego_agent_ts.reward  \n",
        "                if i % 250 == 0:\n",
        "                    action = ego_action_step.action.numpy()\n",
        "                    print(\"Action: {}, \"\n",
        "                          \"Reward: {}\".format(action, episode_return))\n",
        "                i += 1\n",
        "\n",
        "            print(\"Steps in episode: {}\".format(i))\n",
        "            total_return += episode_return\n",
        "        avg_return = total_return / self.num_eval_episodes\n",
        "\n",
        "        if self.use_tensorboard:\n",
        "            with self.tb_file_writer.as_default():\n",
        "                video_summary(\"eval/grid\".format(car_id), self.tb_gif_eval,\n",
        "                              fps=self.FPS, step=self.global_step,\n",
        "                              channels=self.num_channels)\n",
        "\n",
        "            self.tb_gif_eval = np.zeros((self.max_eval_episode_steps,\n",
        "                                        self.num_agents, self.H, self.W,\n",
        "                                         self.stacked_channels))\n",
        "\n",
        "        print(\"Average return: {}\".format(avg_return))\n",
        "\n",
        "\n",
        "        if self.use_separate_agents:  \n",
        "            self.eval_returns[ego_car_index].append(avg_return)\n",
        "        else:  \n",
        "            self.eval_returns.append(avg_return)\n",
        "\n",
        "        return avg_return\n",
        "\n",
        "    def collect_step_lstm(self, step=0, ego_car_index=0, add_to_video=False,\n",
        "                          policy_states=None):\n",
        "        \n",
        "        time_step = self.train_env.current_time_step()\n",
        "\n",
        "        # Create empty list of actions and next policy states\n",
        "        actions = []\n",
        "        next_policy_states = {}\n",
        "\n",
        "        agent_timesteps = self.get_agent_timesteps(time_step, step=step,\n",
        "                                                   only_ego_car=False)\n",
        "\n",
        "        if self.use_separate_agents:\n",
        "            ego_agent_policy = self.collect_policies[ego_car_index]\n",
        "        else:\n",
        "            ego_agent_policy = self.collect_policy\n",
        "\n",
        "        for car_id in range(self.num_agents):\n",
        "            if car_id == ego_car_index:\n",
        "                ego_agent_ts = agent_timesteps[car_id]\n",
        "                if self.use_separate_agents:\n",
        "                    ego_policy_step = ego_agent_policy.action(\n",
        "                        ego_agent_ts, policy_states[car_id])\n",
        "                else:\n",
        "                    ego_policy_step = self.collect_policy.action(ego_agent_ts,\n",
        "                                                                 policy_states[car_id])\n",
        "                if use_greedy:\n",
        "                    actions.append(ego_action_step.info['loc']) \n",
        "                else:\n",
        "                    actions.append(ego_action_step.action)  \n",
        "                policy_state = ego_policy_step.state\n",
        "\n",
        "                if self.add_to_video:\n",
        "                    rendered_state = time_step.observation[:, car_id].numpy()\n",
        "                    if self.num_frames > 1:  \n",
        "                        rendered_state = rendered_state[..., :3] \n",
        "                    self.video_train.append(rendered_state)\n",
        "\n",
        "            elif self.use_separate_agents:\n",
        "                other_agent_ts = agent_timesteps[car_id]\n",
        "                policy_step = self.eval_policies[car_id].action(other_agent_ts, policy_states[car_id])\n",
        "                policy_state = policy_step.state\n",
        "                actions.append(policy_step.action)  \n",
        "\n",
        "            elif self.use_self_play:\n",
        "                other_agent_ts = agent_timesteps[car_id]\n",
        "                policy_step = self.eval_policy.action(other_agent_ts, policy_states[car_id])\n",
        "                policy_state = policy_step.state\n",
        "                actions.append(policy_step.action)\n",
        "\n",
        "            next_policy_states[car_id] = policy_state  \n",
        "\n",
        "        if self.use_tensorboard:\n",
        "            processed_observations = self._process_observations(time_step,\n",
        "                                                                only_ego_car=False)\n",
        "            self.tb_gif_train[step] = tf.convert_to_tensor(processed_observations)\n",
        "\n",
        "        action_tensor = tf.convert_to_tensor([tf.stack(tuple(actions), axis=1)])\n",
        "\n",
        "        next_time_step = self.train_env.step(action_tensor)\n",
        "        ego_agent_next_ts = self.get_agent_timesteps(next_time_step,\n",
        "                                                     step=step + 1,\n",
        "                                                     ego_car_index=ego_car_index,\n",
        "                                                     only_ego_car=True)\n",
        "\n",
        "        traj = trajectory.from_transition(ego_agent_ts, ego_policy_step,\n",
        "                                          ego_agent_next_ts)\n",
        "        self.replay_buffer.add_batch(traj)\n",
        "\n",
        "        if add_to_video:\n",
        "            rendered_state = time_step.observation[:, ego_car_index].numpy()\n",
        "            if self.num_frames > 1:\n",
        "                rendered_state = rendered_state[:, :, :, 3]  \n",
        "            self.video_train.append(rendered_state)\n",
        "\n",
        "        return next_policy_states, float(ego_agent_ts.reward)\n",
        "\n",
        "    def reset_policy_states(self, ego_car_index=0, mode='train'):\n",
        "        \n",
        "        if mode == 'train':\n",
        "            if self.use_separate_agents:\n",
        "                policy_states = {car_id: self.eval_policies[car_id].get_initial_state(\n",
        "                    self.train_env.batch_size) for car_id in range(self.num_agents)}\n",
        "                policy_states[ego_car_index] = self.collect_policies[\n",
        "                    ego_car_index].get_initial_state(self.train_env.batch_size)\n",
        "            else:\n",
        "                policy_states = {car_id: self.eval_policy.get_initial_state(self.train_env.batch_size)\n",
        "                                 for car_id in range(self.num_agents)}\n",
        "                policy_states[ego_car_index] = self.collect_policy.get_initial_state(\n",
        "                    self.train_env.batch_size)\n",
        "\n",
        "        elif mode == 'eval':\n",
        "            if self.use_separate_agents:\n",
        "                policy_states = {\n",
        "                    car_id: self.eval_policies[car_id].get_initial_state(\n",
        "                        self.eval_env.batch_size) for car_id in\n",
        "                    range(self.num_agents)}\n",
        "            else:\n",
        "                policy_states = {car_id: self.eval_policy.get_initial_state(\n",
        "                    self.eval_env.batch_size) for car_id in\n",
        "                    range(self.num_agents)}\n",
        "\n",
        "        return policy_states\n",
        "\n",
        "    def collect_episode_lstm(self, epoch=0, ego_car_index=0, add_to_video=False):\n",
        "        \n",
        "        policy_states = self.reset_policy_states(ego_car_index=ego_car_index)\n",
        "\n",
        "        episode_reward = 0  \n",
        "        step = 0\n",
        "\n",
        "        self.train_env.reset()\n",
        "\n",
        "        use_greedy = float(np.random.binomial(n=1, p=self.epsilon))\n",
        "\n",
        "        while step < self.collect_steps_per_episode and \\\n",
        "                not self.is_last(mode='train'):\n",
        "            if step % 1000 == 0:\n",
        "                print(\"Step number: {}\".format(step))\n",
        "            policy_states, ego_reward = self.collect_step_lstm(add_to_video=add_to_video,\n",
        "                                                               step=step, use_greedy=use_greedy,\n",
        "                                                               ego_car_index=ego_car_index,\n",
        "                                                               policy_states=policy_states)\n",
        "            episode_reward += ego_reward\n",
        "            step += 1\n",
        "\n",
        "        self.global_step += step\n",
        "\n",
        "        if self.use_tensorboard:\n",
        "            with self.tb_file_writer.as_default():\n",
        "                tf.summary.scalar(\"Average Training Reward\", float(episode_reward),\n",
        "                                  step=self.global_step)\n",
        "                frames = self.tb_gif_train\n",
        "                video_summary(\"train/grid\", frames, fps=self.FPS,\n",
        "                              step=self.global_step,\n",
        "                              channels=self.num_channels)\n",
        "\n",
        "            self.tb_gif_train = np.zeros((self.collect_steps_per_episode,\n",
        "                                          self.num_agents, self.H, self.W,\n",
        "                                          self.stacked_channels))\n",
        "\n",
        "    def compute_average_reward_lstm(self, ego_car_index=0):\n",
        "        \n",
        "        total_return = 0.0\n",
        "\n",
        "        for _ in range(self.num_eval_episodes):\n",
        "            time_step = self.eval_env.reset()\n",
        "\n",
        "            i = 0\n",
        "            episode_return = 0.0\n",
        "\n",
        "            policy_states = self.reset_policy_states(ego_car_index=ego_car_index,\n",
        "                                                     mode='eval')\n",
        "\n",
        "            while i < self.max_eval_episode_steps and \\\n",
        "                    not self.is_last(mode='eval'):\n",
        "\n",
        "                actions = []\n",
        "\n",
        "                agent_timesteps = self.get_agent_timesteps(time_step, step=i)\n",
        "                NUM_AGENTS = 1\n",
        "                for car_id in range(NUM_AGENTS):\n",
        "\n",
        "                    if car_id == ego_car_index:\n",
        "                        ego_agent_ts = agent_timesteps[car_id]\n",
        "                        rendered_state = ego_agent_ts.observation.numpy()\n",
        "                        if self.num_frames > 1:\n",
        "                            rendered_state = rendered_state[..., :3]  \n",
        "                        self.video_eval.append(rendered_state)\n",
        "\n",
        "                        if self.use_separate_agents:  \n",
        "                            ego_policy_step = self.eval_policies[car_id].action(ego_agent_ts,\n",
        "                                                                                policy_states[car_id])\n",
        "                            actions.append(ego_policy_step.action)  \n",
        "\n",
        "                        elif self.use_self_play:  \n",
        "                            ego_policy_step = self.eval_policy.action(ego_agent_ts,\n",
        "                                                                      policy_states[car_id])\n",
        "                            actions.append(ego_policy_step.action)  \n",
        "\n",
        "                        policy_state = ego_policy_step.state\n",
        "\n",
        "                    elif self.use_separate_agents:\n",
        "                        other_agent_ts = agent_timesteps[car_id]\n",
        "                        policy_step = self.eval_policies[car_id].action(other_agent_ts,\n",
        "                                                                        policy_states[car_id])\n",
        "                        actions.append(policy_step.action)\n",
        "                        policy_state = policy_step.state\n",
        "\n",
        "                    elif self.use_self_play:\n",
        "                        other_agent_ts = agent_timesteps[car_id]\n",
        "                        policy_step = self.eval_policy.action(other_agent_ts,\n",
        "                                                              policy_states[car_id])\n",
        "                        actions.append(policy_step.action)\n",
        "                        policy_state = policy_step.state\n",
        "\n",
        "                    policy_states[car_id] = policy_state\n",
        "\n",
        "                action_tensor = tf.convert_to_tensor([tf.stack(tuple(actions), axis=1)])\n",
        "\n",
        "                time_step = self.eval_env.step(action_tensor)\n",
        "\n",
        "                if self.use_tensorboard:\n",
        "                    processed_observations = self._process_observations(time_step,\n",
        "                                                                        only_ego_car=False)\n",
        "                    self.tb_gif_eval[i] = tf.convert_to_tensor(processed_observations)\n",
        "\n",
        "                episode_return += ego_agent_ts.reward \n",
        "                if i % 250 == 0:\n",
        "                    action = ego_policy_step.action.numpy()\n",
        "                    print(\"Action: {}, \"\n",
        "                          \"Reward: {}\".format(action, episode_return))\n",
        "                    print(\"POLICY STATES: {}\".format(\n",
        "                        [np.sum(policy_states[i]) for i\n",
        "                         in range(self.num_agents)]))\n",
        "                i += 1\n",
        "            print(\"Steps in episode: {}\".format(i))\n",
        "            total_return += episode_return\n",
        "        avg_return = total_return / self.num_eval_episodes\n",
        "\n",
        "        \n",
        "        print(\"Average return: {}\".format(avg_return))\n",
        "\n",
        "        if self.use_separate_agents:  \n",
        "            self.eval_returns[ego_car_index].append(avg_return)\n",
        "        else: \n",
        "            self.eval_returns.append(avg_return)\n",
        "\n",
        "        return avg_return\n",
        "\n",
        "    def train_agent(self):\n",
        "        \n",
        "        eval_epochs = []\n",
        "\n",
        "        # Optimize by wrapping some of the code in a graph using TF function.\n",
        "        if self.use_separate_agents:\n",
        "            for car_id in range(self.num_agents):\n",
        "                self.agents[car_id].train = common.function(self.agents[car_id].train)\n",
        "                self.agents[car_id].train_step_counter.assign(0)\n",
        "        else:\n",
        "            self.agent.train = common.function(self.agent.train)\n",
        "\n",
        "        # Compute pre-training returns\n",
        "        if self.use_lstm:\n",
        "            avg_return = self.compute_average_reward_lstm(ego_car_index=0)\n",
        "\n",
        "        else:\n",
        "            avg_return = self.compute_average_reward(ego_car_index=0)\n",
        "\n",
        "        # Log average training return to tensorboard\n",
        "        if self.use_tensorboard:\n",
        "            with self.tb_file_writer.as_default():\n",
        "                tf.summary.scalar(\"Average Eval Reward\", float(avg_return),\n",
        "                                  step=self.global_step)\n",
        "\n",
        "        print(\"DONE WITH PRELIMINARY EVALUATION...\")\n",
        "\n",
        "        # Append for output plot, create video, and empty eval video array\n",
        "        eval_epochs.append(0)\n",
        "        self.create_video(mode='eval', ext=0)\n",
        "        self.video_eval = []  # Empty to create a new eval video\n",
        "        returns = [avg_return]\n",
        "\n",
        "        # Reset the environment time step and global and episode step counters\n",
        "#        time_step = self.train_env.reset()\n",
        "        step = 0\n",
        "        i = 0\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            if self.global_step >= self.total_steps:\n",
        "                print(\"Reached the end of training with {} training steps\".format(self.global_step))\n",
        "                break\n",
        "\n",
        "            ego_car_index = i % self.num_agents\n",
        "            print(\"Training epoch: {}\".format(i))\n",
        "\n",
        "            print(\"Collecting episode for car with ID {}\".format(ego_car_index))\n",
        "\n",
        "            self.video_train = []\n",
        "\n",
        "            if self.use_lstm:\n",
        "                self.collect_episode_lstm(epoch=i, ego_car_index=ego_car_index)\n",
        "                print(\"LSTM\")\n",
        "\n",
        "            else:\n",
        "                self.collect_episode(epoch=i, ego_car_index=ego_car_index)\n",
        "                print(\"No LSTM\")\n",
        "\n",
        "            if i % 100 == 0 and self.add_to_video:\n",
        "                self.create_video(mode='train', ext=i)  \n",
        "            print(\"Collected Episode\")\n",
        "\n",
        "            if self.use_gpu:\n",
        "                device = '/gpu:0'\n",
        "            else:\n",
        "                device = '/cpu:0'\n",
        "\n",
        "            with tf.device(device):\n",
        "\n",
        "                trajectories = self.replay_buffer.gather_all()\n",
        "\n",
        "                if self.use_separate_agents:\n",
        "\n",
        "                    train_loss = self.agents[ego_car_index].train(experience=trajectories)\n",
        "\n",
        "                    if self.use_tensorboard:\n",
        "                        with self.tb_file_writer.as_default():\n",
        "                            tf.summary.scalar(\"Training Loss Agent {}\".format(ego_car_index),\n",
        "                                              float(train_loss.loss),\n",
        "                                              step=self.global_step // self.num_agents)\n",
        "\n",
        "                    step = self.agents[ego_car_index].train_step_counter.numpy()\n",
        "\n",
        "\n",
        "                else:\n",
        "                    train_loss = self.agent.train(experience=trajectories)\n",
        "\n",
        "                    if self.use_tensorboard:\n",
        "                        with self.tb_file_writer.as_default():\n",
        "                            tf.summary.scalar(\"Training Loss\",\n",
        "                                              float(train_loss.loss), step=self.global_step)\n",
        "\n",
        "            with tf.device('/cpu:0'):\n",
        "\n",
        "                if self.global_step % self.log_interval == 0:\n",
        "                    print('step = {0}: loss = {1}'.format(self.global_step,\n",
        "                                                          train_loss.loss))\n",
        "\n",
        "                if i % self.eval_interval == 0:\n",
        "\n",
        "                    if self.use_lstm:\n",
        "                        avg_return = self.compute_average_reward_lstm(ego_car_index=ego_car_index)\n",
        "                    else:\n",
        "                        avg_return = self.compute_average_reward(ego_car_index=ego_car_index)\n",
        "\n",
        "                    if self.use_tensorboard:\n",
        "                        with self.tb_file_writer.as_default():\n",
        "                            tf.summary.scalar(\"Average Eval Reward\",\n",
        "                                              float(avg_return),\n",
        "                                              step=self.global_step)\n",
        "                    eval_epochs.append(i + 1)\n",
        "                    print(\n",
        "                        'epoch = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "                    returns.append(avg_return)\n",
        "                    if self.add_to_video:\n",
        "                        self.create_video(mode='eval', ext=i)\n",
        "                    self.video_eval = []  \n",
        "\n",
        "                if i % self.save_interval == 0 and i != 0:\n",
        "                    self.save_policies(epochs_done=i)\n",
        "                    print(\"Epochs: {}\".format(i))\n",
        "\n",
        "                self.replay_buffer.clear()\n",
        "\n",
        "        if self.use_separate_agents:\n",
        "            return self.agents\n",
        "        else:\n",
        "            return self.agent\n",
        "\n",
        "    def create_video(self, mode='eval', ext=0, ego_car_index=0):\n",
        "        \n",
        "        if mode == 'eval':  \n",
        "            video = self.video_eval\n",
        "        elif mode == 'train':  \n",
        "            video = self.video_train\n",
        "\n",
        "        if len(video) == 0:\n",
        "            raise AssertionError(\"Video is empty.\")\n",
        "        print(\"Number of frames in video: {}\".format(len(video)))\n",
        "        obs_size = video[0].shape\n",
        "        width = np.uint(obs_size[-3])\n",
        "        height = np.uint(obs_size[-2])\n",
        "        channels = np.uint(obs_size[-1])\n",
        "        print(\"HEIGHT IS: {}, WIDTH IS: {}, CHANNELS IS: {}\".format(width, height, channels))\n",
        "\n",
        "        fourcc = cv.VideoWriter_fourcc(*'XVID')\n",
        "        out_file = os.path.join(self.log_dir,\n",
        "                                \"trajectories_{}_epoch_{}_agent_{}\"\n",
        "                                \".avi\".format(mode, ext, ego_car_index))\n",
        "        out = cv.VideoWriter(out_file, fourcc, self.FPS, (width, height))\n",
        "\n",
        "        for i in range(len(video)):\n",
        "            img_rgb = cv.cvtColor(np.uint8(255 * video[i][0]),\n",
        "                                  cv.COLOR_BGR2RGB)  # Save as RGB image\n",
        "            out.write(img_rgb)\n",
        "        out.release()\n",
        "\n",
        "    def plot_eval(self):\n",
        "    \n",
        "        if self.use_separate_agents:  \n",
        "            for car_id in range(self.num_agents):\n",
        "                xs = [i * self.eval_interval for\n",
        "                      i in range(len(self.eval_returns[car_id]))]\n",
        "                plt.plot(xs, self.eval_returns[car_id])\n",
        "                plt.xlabel(\"Training epochs\")\n",
        "                plt.ylabel(\"Average Return\")\n",
        "                plt.title(\"Average Returns as a Function \"\n",
        "                          \"of Training (Agent {})\".format(car_id))\n",
        "                save_path = os.path.join(self.policy_save_dir,\n",
        "                                         \"eval_returns_agent_{}\"\n",
        "                                         \".png\".format(car_id))\n",
        "                plt.savefig(save_path)\n",
        "                print(\"Created plot of returns for agent {}...\".format(car_id))\n",
        "\n",
        "        else:\n",
        "            xs = [i * self.eval_interval for i in range(len(self.eval_returns))]\n",
        "            plt.plot(xs, self.eval_returns)\n",
        "            plt.xlabel(\"Training epochs\")\n",
        "            plt.ylabel(\"Average Return\")\n",
        "            plt.title(\"Average Returns as a Function of Training\")\n",
        "            save_path = os.path.join(self.policy_save_dir, \"eval_returns.png\")\n",
        "            plt.savefig(save_path)\n",
        "            print(\"CREATED PLOT OF RETURNS\")\n",
        "\n",
        "\n",
        "    def save_policies(self, epochs_done=0, is_final=False):\n",
        "       \n",
        "        if is_final:\n",
        "            epochs_done = \"FINAL\"\n",
        "\n",
        "        if self.use_separate_agents:\n",
        "\n",
        "            for i, train_saver in enumerate(self.train_savers):\n",
        "                if custom_path is None:\n",
        "                    train_save_dir = os.path.join(self.policy_save_dir, \"train\",\n",
        "                                                 \"epochs_{}\".format(epochs_done),\n",
        "                                                 \"agent_{}\".format(i))\n",
        "                else:\n",
        "                    train_save_dir = os.path.join(self.policy_save_dir, \"train\",\n",
        "                                                  \"epochs_{}\".format(\n",
        "                                                      custom_path),\n",
        "                                                  \"agent_{}\".format(i))\n",
        "                if not os.path.exists(train_save_dir):\n",
        "                    os.makedirs(train_save_dir, exist_ok=True)\n",
        "                train_saver.save(train_save_dir)\n",
        "\n",
        "            print(\"Training policies saved...\")\n",
        "\n",
        "            for i, eval_saver in enumerate(self.eval_savers):\n",
        "                eval_save_dir = os.path.join(self.policy_save_dir, \"eval\",\n",
        "                                             \"epochs_{}\".format(epochs_done),\n",
        "                                             \"agent_{}\".format(i))\n",
        "                if not os.path.exists(eval_save_dir):\n",
        "                    os.makedirs(eval_save_dir, exist_ok=True)\n",
        "                eval_saver.save(eval_save_dir)\n",
        "\n",
        "            print(\"Eval policies saved...\")\n",
        "\n",
        "        else:\n",
        "            train_save_dir = os.path.join(self.policy_save_dir, \"train\",\n",
        "                                          \"epochs_{}\".format(epochs_done))\n",
        "            if not os.path.exists(train_save_dir):\n",
        "                os.makedirs(train_save_dir, exist_ok=True)\n",
        "            self.train_saver.save(train_save_dir)\n",
        "\n",
        "            print(\"Training policy saved...\")\n",
        "\n",
        "            # Save eval policy\n",
        "            eval_save_dir = os.path.join(self.policy_save_dir, \"eval\",\n",
        "                                         \"epochs_{}\".format(epochs_done))\n",
        "            if not os.path.exists(eval_save_dir):\n",
        "                os.makedirs(eval_save_dir, exist_ok=True)\n",
        "            self.eval_saver.save(eval_save_dir)\n",
        "\n",
        "            print(\"Eval policy saved...\")\n",
        "\n",
        "        agent_params = {'normalize_obs': self.train_env.normalize,\n",
        "                        'use_lstm': self.use_lstm,\n",
        "                        'frame_stack': self.use_multiple_frames,\n",
        "                        'num_frame_stack': self.env.num_frame_stack,\n",
        "                        'obs_size': self.size}\n",
        "\n",
        "        params_path = os.path.join(self.policy_save_dir, \"parameters.pkl\")\n",
        "        with open(params_path, \"w\") as pkl_file:\n",
        "            pickle.dump(agent_params, pkl_file)\n",
        "        pkl_file.close()\n",
        "\n",
        "    def load_saved_policies(self, eval_model_path=None, train_model_path=None):\n",
        "    \n",
        "        if eval_model_path is not None:\n",
        "            self.eval_policy = tf.saved_model.load(eval_model_path)\n",
        "            print(\"Loading evaluation policy from: {}\".format(eval_model_path))\n",
        "\n",
        "        if train_model_path is not None:\n",
        "            self.collect_policy = tf.saved_model.load(train_model_path)\n",
        "            print(\"Loading training policy from: {}\".format(train_model_path))\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    \"\"\"Argument-parsing function for running this code.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"-n\", \"--num_agents\", default=2, type=int,\n",
        "                        help=\"Number of cars in the environment.\")\n",
        "    parser.add_argument(\"-size\", \"--size\", required=False,\n",
        "                        default=\"96\",\n",
        "                        help=\"The width and height of the observation window.\")\n",
        "    parser.add_argument(\"-direction\", \"--direction\", type=str, default='CCW',\n",
        "                        help=\"Direction in which agents traverse the track.\")\n",
        "    parser.add_argument(\"-random_direction\", \"--use_random_direction\",\n",
        "                        required=False, action='store_true',\n",
        "                        help=\"Whether agents are trained/evaluated on \"\n",
        "                             \"both CW and CCW trajectories across the track.\")\n",
        "    parser.add_argument(\"-backwards_flag\", \"--backwards_flag\", required=False,\n",
        "                        action=\"store_true\",\n",
        "                        help=\"Whether to render a backwards flag indicator when \"\n",
        "                             \"an agent drives on the track backwards.\")\n",
        "    parser.add_argument(\"-h_ratio\", \"--h_ratio\", type=float, default=0.25,\n",
        "                        help=\"Default height location fraction for where car\"\n",
        "                             \"is located in observation upon rendering.\")\n",
        "    parser.add_argument(\"-ego_color\", \"--use_ego_color\", required=False,\n",
        "                        action=\"store_true\", default=\"Whether to render each \"\n",
        "                                                     \"ego car in the same color.\")\n",
        "\n",
        "    parser.add_argument(\"-self_play\", \"--use_self_play\",\n",
        "                        required=False, action=\"store_true\",\n",
        "                        help=\"Flag for whether to use a single master PPO agent.\")\n",
        "    parser.add_argument(\"-n_agents\", \"--use_separate_agents\",\n",
        "                        required=False, action=\"store_true\",\n",
        "                        help=\"Flag for whether to use a N PPO agents.\")\n",
        "\n",
        "    parser.add_argument(\"-epochs\", \"--total_epochs\", default=1000, type=int,\n",
        "                        help=\"Number of epochs to train agent over.\")\n",
        "    parser.add_argument(\"-steps\", \"--total_steps\", type=int, default=10e6,\n",
        "                        help=\"Total number of training steps to take.\")\n",
        "    parser.add_argument(\"-collect_episode_steps\", \"--collect_steps_per_episode\",\n",
        "                        default=1000, type=int,\n",
        "                        help=\"Number of steps to take per collection episode.\")\n",
        "    parser.add_argument(\"-eval_episode_steps\", \"--eval_steps_per_episode\",\n",
        "                        default=1000, type=int,\n",
        "                        help=\"Number of steps to take per evaluation episode.\")\n",
        "    parser.add_argument(\"-eval_interval\", \"--eval_interval\", default=10,\n",
        "                        type=int,\n",
        "                        help=\"Evaluate every time epoch % eval_interval = 0.\")\n",
        "    parser.add_argument(\"-eval_episodes\", \"--num_eval_episodes\", default=5,\n",
        "                        type=int,\n",
        "                        help=\"Evaluate over eval_episodes evaluation episodes.\")\n",
        "    parser.add_argument(\"-lr\", \"--learning_rate\", default=5e-8, type=float,\n",
        "                        help=\"Learning rate for PPO agent(s).\")\n",
        "    parser.add_argument(\"-lstm\", \"--use_lstm\", required=False, action=\"store_true\",\n",
        "                        help=\"Flag for whether to use LSTMs on actor and critic\"\n",
        "                             \"networks of the PPO agent.\")\n",
        "    parser.add_argument(\"-eps\", \"--epsilon\", type=float, default=0.0,\n",
        "                        help=\"Probability of training on the greedy policy for a\"\n",
        "                             \"given episode\")\n",
        "\n",
        "    parser.add_argument(\"-si\", \"--save_interval\", default=10, type=int,\n",
        "                        help=\"Save policies every time epoch % eval_interval = 0.\")\n",
        "    parser.add_argument(\"-li\", \"--log_interval\", default=1, type=int,\n",
        "                        help=\"Log results every time epoch % eval_interval = 0.\")\n",
        "    parser.add_argument(\"-tb\", \"--use_tensorboard\", required=False,\n",
        "                        action=\"store_true\", help=\"Log with tensorboard as well.\")\n",
        "    parser.add_argument(\"-add_to_video\", \"--add_to_video\", required=False,\n",
        "                        action=\"store_true\",\n",
        "                        help=\"Whether to save trajectories as videos.\")\n",
        "\n",
        "    parser.add_argument(\"-exp_name\", \"--experiment_name\", type=str,\n",
        "                        default=\"experiment_{}\", required=False,\n",
        "                        help=\"Name of experiment (for logging).\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"Your selected training parameters: \\n {}\".format(vars(args)))\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "  main()\n",
        "trainer = PPOTrainer(u, t, y, size=(96, 96),\n",
        "                     normalize=True, num_frames=1, num_channels=3,\n",
        "                     use_tensorboard=True, add_to_video=True,\n",
        "                     use_separate_agents=False, use_self_play=False,\n",
        "                     num_agents=2, use_lstm=False, experiment_name=\"\",\n",
        "                     collect_steps_per_episode=1000, total_epochs=1000,\n",
        "                     total_steps=1e6, eval_steps_per_episode=1000,\n",
        "                     eval_interval=100, num_eval_episodes=5, epsilon=0.0,\n",
        "                     save_interval=500, log_interval=1)\n",
        "\n",
        "trainer.create_video(mode='eval', ext=0, ego_car_index=0)\n",
        "\n",
        "\n",
        "print(\"Initialized agent, beginning training...\")"
      ],
      "metadata": {
        "id": "1xwCOKKnwpFw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "7c906e67-e64a-4ff8-d156-bb8418880c4a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total steps: 1000000.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-c89a2fbba20c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    974\u001b[0m                      \u001b[0mtotal_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_steps_per_episode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m                      \u001b[0meval_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                      save_interval=500, log_interval=1)\n\u001b[0m\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mego_car_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-c89a2fbba20c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ppo_agents, train_env, eval_env, size, normalize, num_frames, num_channels, use_tensorboard, add_to_video, use_separate_agents, use_self_play, num_agents, use_lstm, experiment_name, collect_steps_per_episode, total_epochs, total_steps, eval_steps_per_episode, eval_interval, num_eval_episodes, epsilon, save_interval, log_interval)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actor_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute '_actor_net'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXYWtk4MLDTI"
      },
      "source": [
        "# КОД из ноутбука Optima_1\n",
        "@author: Vadim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install control"
      ],
      "metadata": {
        "id": "JowWYy9k6nqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# optimal_bench.py - benchmarks for optimal control package\n",
        "# RMM, 27 Feb 2020\n",
        "#\n",
        "# This benchmark tests the timing for the optimal control module\n",
        "# (control.optimal) and is intended to be used for helping tune the\n",
        "# performance of the functions used for optimization-base control.\n",
        "\n",
        "import math\n",
        "import control as ct\n",
        "import control.flatsys as flat\n",
        "import control.optimal as opt\n",
        "import logging\n",
        "\n",
        "\n",
        "#\n",
        "# Vehicle steering dynamics\n",
        "#\n",
        "# The vehicle dynamics are given by a simple bicycle model.  We take the state\n",
        "# of the system as (x, y, theta) where (x, y) is the position of the vehicle\n",
        "# in the plane and theta is the angle of the vehicle with respect to\n",
        "# horizontal.  The vehicle input is given by (v, phi) where v is the forward\n",
        "# velocity of the vehicle and phi is the angle of the steering wheel.  The\n",
        "# model includes saturation of the vehicle steering angle.\n",
        "#\n",
        "# System state: x, y, theta\n",
        "# System input: v, phi\n",
        "# System output: x, y\n",
        "# System parameters: wheelbase, maxsteer\n",
        "#\n",
        "def vehicle_update(t, x, u, params):\n",
        "    # Get the parameters for the model\n",
        "    l = params.get('wheelbase', 3.)         # vehicle wheelbase\n",
        "    phimax = params.get('maxsteer', 0.5)    # max steering angle (rad)\n",
        "    # Saturate the steering input (use min/max instead of clip for speed)\n",
        "    phi = max(-phimax, min(u[1], phimax))\n",
        "    # Return the derivative of the state\n",
        "    return np.array([\n",
        "        math.cos(x[2]) * u[0],            # xdot = cos(theta) v\n",
        "        math.sin(x[2]) * u[0],            # ydot = sin(theta) v\n",
        "        (u[0] / l) * math.tan(phi)        # thdot = v/l tan(phi)\n",
        "    ])\n",
        "\n",
        "\n",
        "def vehicle_output(t, x, u, params):\n",
        "    return x                            # return x, y, theta (full state)\n",
        "\n",
        "vehicle = ct.NonlinearIOSystem(\n",
        "    vehicle_update, vehicle_output, states=3, name='vehicle',\n",
        "    inputs=('v', 'phi'), outputs=('x', 'y', 'theta'))\n",
        "\n",
        "# Initial and final conditions\n",
        "x0 = [0., -2., 0.]; u0 = [10., 0.] #начальное положение автомобиля\n",
        "xf = [120., 1.1, 0.]; uf = [10., 0.] #конечное положение автомобиля\n",
        "Tf = 12.14 # увеличение значения приводит к изменению траектории (см.первый график)\n",
        "# траектория укорачивается при уменьшении значения Tf\n",
        "# запомнить значение 12.14\n",
        "\n",
        "# Define the time horizon (and spacing) for the optimization\n",
        "horizon = np.linspace(0, Tf, 10, endpoint=True)\n",
        "# Provide an intial guess (will be extended to entire horizon)\n",
        "bend_left = [10.9405, 0.01]          # slight left veer\n",
        "\n",
        "# Set up the cost functions\n",
        "Q = np.diag([5, 10, 1])     # keep lateral error low\n",
        "R = np.diag([.1, 1])          # minimize applied inputs\n",
        "quad_cost = opt.quadratic_cost(vehicle, Q, R, x0=xf, u0=uf)\n",
        "#        \n",
        "res = opt.solve_ocp(\n",
        "        vehicle, horizon, x0, quad_cost,\n",
        "        initial_guess=bend_left, print_summary=False,\n",
        "        # solve_ivp_kwargs={'atol': 1e-2, 'rtol': 1e-2},\n",
        "        minimize_method='trust-constr',\n",
        "        minimize_options={'finite_diff_rel_step': 0.01},\n",
        "    )\n",
        "# функция u зависящая от угол поворота и скорость\n",
        "# выходные параметры: время, координата y\n",
        "u = res.inputs\n",
        "t, y = ct.input_output_response(vehicle, horizon, u, x0) \n",
        "#y = res.states\n",
        "#t = res.time\n",
        "\n",
        "#print('выходная функция?:\\n', res)\n",
        "#print('Время t:\\n', t)\n",
        "print('Функция g(х,y):\\n', y)\n",
        "print('Функция u(скорость, угол):\\n', u)\n",
        "\n",
        "plt.figure(figsize=(10, 10)) # увеличение размера графиков\n",
        "\n",
        "# Plot the results\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(y[0], y[1])\n",
        "plt.plot(x0[0], x0[1], 'ro', xf[0], xf[1], 'ro')\n",
        "plt.xlabel(\"Координата x [m]\")\n",
        "plt.ylabel(\"Координата y [m]\")\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(t, u[0])\n",
        "#plt.axis([0, 10, 5.5, 22.5])\n",
        "plt.axis([0, 10, 8.9, 11.5])\n",
        "plt.plot([0, 10], [9, 9], 'k--', [0, 10], [11, 11], 'k--') # установка диапазона значений\n",
        "plt.xlabel(\"Время t [sec]\")\n",
        "plt.ylabel(\"Скорость u1 [m/s]\")\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(t, u[1])\n",
        "#plt.axis([0, 10, -0.35, 0.35])\n",
        "plt.axis([0, 10, -0.25, 0.25])\n",
        "plt.plot([0, 10], [-0.1, -0.1], 'k--', [0, 10], [0.1, 0.1], 'k--') # установка диапазона значений\n",
        "plt.xlabel(\"Время t [sec]\")\n",
        "plt.ylabel(\"Угловая скорость u2 [rad/s]\")\n",
        "\n",
        "plt.suptitle(\"Траектория движения\") #Lane change manuever\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ady0byiAx-pi",
        "outputId": "55f87cdb-19f4-4911-d51f-974613ec594f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary statistics:\n",
            "* Cost function calls: 672\n",
            "* System simulations: 672\n",
            "Функция g(х,y):\n",
            " [[ 0.00000000e+00  1.23350952e+01  2.46702150e+01  3.70018424e+01\n",
            "   4.93261422e+01  6.16483229e+01  7.39647403e+01  8.62846613e+01\n",
            "   9.86032506e+01  1.10914016e+02]\n",
            " [-2.00000000e+00 -1.91395228e+00 -1.74455529e+00 -1.48176215e+00\n",
            "  -1.03059265e+00 -5.57548690e-01  6.74334633e-02  5.13410261e-01\n",
            "   9.68215251e-01  1.58673416e+00]\n",
            " [ 0.00000000e+00  1.15923886e-02  1.59737436e-02  3.00326564e-02\n",
            "   3.71879786e-02  4.60635690e-02  4.53629413e-02  3.28818201e-02\n",
            "   4.16246335e-02  6.16423900e-02]]\n",
            "Функция u(скорость, угол):\n",
            " [[ 1.09472159e+01  1.09500924e+01  1.09487243e+01  1.09469644e+01\n",
            "   1.09451097e+01  1.09435617e+01  1.09423310e+01  1.09414696e+01\n",
            "   1.09408266e+01  1.09405400e+01]\n",
            " [ 4.55904588e-03  1.05656885e-03  7.66551528e-04  5.80152160e-03\n",
            "  -2.55249760e-03  7.26145925e-03 -7.69583291e-03  1.55397560e-03\n",
            "   2.82752788e-03  6.96618441e-03]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# КОД из ноутбука VIS_1\n",
        "@author: Vadim"
      ],
      "metadata": {
        "id": "aYbUGzK0xW2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#изменение исходного кода ноутбука VIS_1\n",
        "установка дополнительных библиотек, позволяющих корректно установить OPENAI GYM в Google Colab\n",
        "\n",
        "полная документация по настройке библиотеки OpenAI GYM:\n",
        "http://gym.openai.com/docs/\n",
        "\n",
        "ноутбук с разобранной настройкой https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb#scrollTo=RlO7WIQHu_7D\n",
        "\n",
        "ПС:\n",
        "- вместо gym устанавливается suite_gym из tf_agents.environments\n"
      ],
      "metadata": {
        "id": "E99CW-v58DFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "metadata": {
        "id": "5q_AdMWo_zR8"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.version.VERSION"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AmIkx5YA_2Hj",
        "outputId": "2f71d50b-6d95-4f02-a895-88600dc34763"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'CarRacing-v0'\n",
        "env = suite_gym.load(env_name)\n",
        "\n",
        "env.reset()\n",
        "for _ in range(1000):\n",
        "    env.render()\n",
        "    PIL.Image.fromarray(env.render())\n",
        "    env.step(env.action_space.sample())\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63N3wxqhF8R5",
        "outputId": "bc91c26c-b765-4fd8-8412-918b3297195f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Track generation: 1103..1392 -> 289-tiles track\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PIL.Image.fromarray(env.render())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "BOyGVJFrmbb0",
        "outputId": "5a420644-06c8-471f-f7ea-8f6d5efa2e77"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=600x400 at 0x7FD779008CD0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модель (настройка TensorFlow)"
      ],
      "metadata": {
        "id": "JzjSjRSes0-q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc2HAOLhwS9o"
      },
      "source": [
        "# Разбиваем датафрейм на части, необходимые для обучения и тестирования модели  \n",
        "# x - данные с информацией о (x, y), у - целевая переменная (u) \n",
        "# x - вход в модель; y - выход модели\n",
        "\n",
        "# y[0], y[1] - x и y\n",
        "# u[0], u[1] - скорость и угловая скорость(ориентация автомобиля)\n",
        "# t - время t\n",
        "\n",
        "x = np.array(u)\n",
        "y1 = np.array(t)\n",
        "y=y1.transpose()"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN01uAu_xYQw",
        "outputId": "47bc59a8-fe60-4730-8540-dbfe7916c11a"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.          1.34888889  2.69777778  4.04666667  5.39555556  6.74444444\n",
            "  8.09333333  9.44222222 10.79111111 12.14      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h8BJNebyduY",
        "outputId": "fac3d9e4-3b65-4bd5-e2e2-316114ef40db"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.09413755e+01  1.09419106e+01  1.09414488e+01  1.09413525e+01\n",
            "   1.09409246e+01  1.09407626e+01  1.09405797e+01  1.09405225e+01\n",
            "   1.09404652e+01  1.09404855e+01]\n",
            " [ 3.80443368e-03  5.81805311e-03  8.76031705e-04  1.54404102e-05\n",
            "  -5.11460662e-03 -6.98676601e-04 -2.13240617e-03  5.14152267e-03\n",
            "   7.99905895e-03  9.82704335e-03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "wcUBK0EPxdhi",
        "outputId": "1b4cbb86-16e0-4398-d17b-6aff30519696"
      },
      "source": [
        "#строим и обучаем модель\n",
        "#используем Activation = relu - получается выше точность\n",
        "# об Activation - https://faroit.com/keras-docs/1.2.0/activations/ \n",
        "# * при количестве слоев: 20 и 20 и 1 и 100 - 0,1424-0,1426\n",
        "# при количестве слоев: 50 и 20 - 0,1416-0,1420\n",
        "# при количестве слоев: 20 и 50 - 0,1416-0,1420\n",
        "# при количестве слоев: 50 и 50 и 3 - 0,1415-0,1420\n",
        "# при количестве слоев: 20 и 20 и 3 - 0,1415-0,1420\n",
        "# при количестве слоев: 20 и 10 и 3 и 50 - 0,1416-0,1419\n",
        "# при количестве слоев: 20 и 20 и 10 и 1 и 50 - 0,1423-0,1425\n",
        "\n",
        "\n",
        "#model = keras.Sequential()\n",
        "#model.add(LSTM(output_dim=300, input_shape=x_train.shape[1:], return_sequences=True, \n",
        "#               init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
        "\n",
        "model = keras.Sequential([Dense(units=20, input_shape=(10,), activation='relu'), \n",
        "                          Dense(units=20, activation='sigmoid'),\n",
        "                          Dense(units=30, activation='sigmoid'),\n",
        "#                          Dense(units=10, activation='sigmoid'),                          \n",
        "                          Dense(units=2, activation='sigmoid')\n",
        "                         ])\n",
        "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(0.1))\n",
        "log = model.fit(x[0], y, batch_size=32, epochs=50, verbose=False)\n",
        "plt.plot(log.history['loss'])\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None,).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-c3d51154c290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                          ])\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 227, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential\" (type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02f79d7f"
      },
      "source": [
        "d=model.predict(x)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96aa8260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac8a485b-1bb9-4e8b-a1e9-1b74ff8f0b53"
      },
      "source": [
        "print(d)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5427421  0.23010191]\n",
            " [0.53472537 0.23624611]]\n"
          ]
        }
      ]
    }
  ]
}