{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FIT-M_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fg7pzJ1dtUX"
      },
      "source": [
        "# РЕШЕНИЕ ЗАДАЧИ \"ТРАССА: оптимальная траектория\"\n",
        "\n",
        "Требуется определить оптимальную траекторию при движении гоночной машины по трассе. \n",
        "\n",
        "Параметры управления: угол ориентации машины и её скорость\n",
        "Параметры системы: координаты x и y\n",
        "Выходной параметр: время движения по трассе (т.е. скорость должна быть как можно выше)\n",
        "Критерий оптимизации: минимальное расстояние между предложенной трассой и расчетной трассой\n",
        "\n",
        "Трасса определяется координатами x и y.\n",
        "\n",
        "Описание задачи доступно по ссылке: https://drive.google.com/file/d/13k3FiNV7XmCUuIKt-00d8ua5r5QVYpBK/view?usp=sharing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4J0bF8Ndrn1"
      },
      "source": [
        "**Загрузка библиотек**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-4NVHKcgtq_"
      },
      "source": [
        "# УСТАНОВКА БИБЛИОТЕК\n",
        "\n",
        "!pip install -q keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "Td8IOAzbkcwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# установка tf-Agent как в руководстве\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet"
      ],
      "metadata": {
        "id": "UuHmHY8_uhYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[all]\n",
        "!pip install gym[box2d]\n",
        "#!pip install gym atari-py\n",
        "#!pip install Box2D\n",
        "#!pip install box2d-kengz"
      ],
      "metadata": {
        "id": "vLUQwqIU9e3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFPWrLLVdq1b"
      },
      "source": [
        "# ИМПОРТ БИБЛИОТЕК\n",
        "\n",
        "import os\n",
        "\n",
        "#numpy & keras\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "# для визуализации данных\n",
        "import cv2 as cv\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "%matplotlib inline\n",
        "\n",
        "# библиотека для обработки даты, времени\n",
        "import datetime as dt\n",
        "import time\n",
        "from datetime import datetime, date, time, timedelta\n",
        "\n",
        "#\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#tensorflow & keras\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "3jhuJG9UvSd1",
        "outputId": "fa38dc3d-9931-4b40-b561-66d3ccd4283e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (np.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzT5-M9U4QNm",
        "outputId": "36c62016-d6a1-4993-931b-adb7f8e1e784"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.19.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# из Notebook @author: Vadim Created on Thu Dec 16 15:50:11 2021\n",
        "import argparse #argparse - это модуль для обработки аргументов командной строки\n",
        "import pickle #pickle - Модуль pickle предоставляет функции и классы для сериализации и десериализации объектов\n",
        "\n",
        "# \n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "import tf_agents.trajectories.time_step as ts\n",
        "\n",
        "from tf_agents.policies.policy_saver import PolicySaver\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "import re"
      ],
      "metadata": {
        "id": "au8ldV3Bv03U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# КЛАСС из ноутбука Notebook\n",
        "@author: Vadim"
      ],
      "metadata": {
        "id": "esEPEOWXwfWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOTrainer:\n",
        "    \n",
        "    def __init__(self, ppo_agents, train_env, eval_env, size=(96, 96),\n",
        "                 normalize=True, num_frames=1, num_channels=3,\n",
        "                 use_tensorboard=True, add_to_video=True,\n",
        "                 use_separate_agents=False, use_self_play=False,\n",
        "                 num_agents=2, use_lstm=False, experiment_name=\"\",\n",
        "                 collect_steps_per_episode=1000, total_epochs=1000,\n",
        "                 total_steps=1e6, eval_steps_per_episode=1000,\n",
        "                 eval_interval=100, num_eval_episodes=5, epsilon=0.0,\n",
        "                 save_interval=500, log_interval=1):\n",
        "\n",
        "        self.train_env = train_env\n",
        "        self.eval_env = eval_env  \n",
        "\n",
        "        self.size = size\n",
        "        self.H, self.W = self.size[0], self.size[1]  \n",
        "        self.normalize = normalize\n",
        "        self.num_frames = num_frames\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        self.use_separate_agents = use_separate_agents  \n",
        "        self.use_self_play = use_self_play  \n",
        "        self.use_lstm = use_lstm  \n",
        "        self.num_agents = num_agents\n",
        "\n",
        "        self.max_buffer_size = collect_steps_per_episode \n",
        "        self.collect_steps_per_episode = collect_steps_per_episode  \n",
        "        self.epochs = total_epochs  \n",
        "        self.total_steps = total_steps  \n",
        "        self.global_step = 0  \n",
        "        self.epsilon = epsilon \n",
        "\n",
        "        print(\"Total steps: {}\".format(self.total_steps))\n",
        "\n",
        "        # Create N different PPO agents\n",
        "        if use_separate_agents and self.num_agents > 1:\n",
        "            self.agents = ppo_agents  \n",
        "            for agent in self.agents:\n",
        "                agent.initialize()  \n",
        "            self.actor_nets = [self.agents[i]._actor_net \\\n",
        "                               for i in range(self.num_agents)]\n",
        "            self.value_nets = [self.agents[i]._value_net \\\n",
        "                               for i in range(self.num_agents)]\n",
        "            self.eval_policies = [self.agents[i].policy \\\n",
        "                                  for i in range(self.num_agents)]\n",
        "            self.collect_policies = [self.agents[i].collect_policy \\\n",
        "                                     for i in range(self.num_agents)]\n",
        "            self.replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "                self.agents[0].collect_data_spec,\n",
        "                batch_size=self.train_env.batch_size,\n",
        "                max_length=self.max_buffer_size)  # Create shared replay buffer\n",
        "\n",
        "        else:\n",
        "            self.agent = ppo_agents\n",
        "            self.actor_net = self.agent._actor_net\n",
        "            self.value_net = self.agent._value_net\n",
        "            self.eval_policy = self.agent.policy\n",
        "            self.collect_policy = self.agent.collect_policy\n",
        "            self.replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "                self.agent.collect_data_spec,\n",
        "                batch_size=self.train_env.batch_size,\n",
        "                max_length=self.max_buffer_size)\n",
        "\n",
        "        if self.num_agents > 1:  \n",
        "            self.observation_wrappers = \\\n",
        "                            [ObservationWrapper(size=self.size, normalize=self.normalize,\n",
        "                                                num_channels=self.num_channels,\n",
        "                                                num_frames=self.num_frames)\n",
        "                             for i in range(self.num_agents)]\n",
        "\n",
        "        else:  # Single observation wrapper for single car\n",
        "            self.observation_wrapper = ObservationWrapper(size=self.size,\n",
        "                                                          normalize=self.normalize,\n",
        "                                                          num_channels=self.num_channels,\n",
        "                                                          num_frames=self.num_frames)\n",
        "\n",
        "        # Evaluation\n",
        "        self.num_eval_episodes = num_eval_episodes  \n",
        "\n",
        "        if self.use_separate_agents:\n",
        "            self.eval_returns = [[] for i in range(self.num_agents)]\n",
        "\n",
        "        else:\n",
        "            self.eval_returns = []\n",
        "\n",
        "        self.eval_interval = eval_interval  \n",
        "        self.max_eval_episode_steps = eval_steps_per_episode  \n",
        "        self.time_ext = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "        self.log_interval = log_interval\n",
        "        self.video_train = []\n",
        "        self.video_eval = []\n",
        "        self.add_to_video = add_to_video\n",
        "        self.FPS = 50  \n",
        "        self.policy_save_dir = os.path.join(os.path.split(__file__)[0], \"models\",\n",
        "                                            experiment_name.format(self.time_ext))\n",
        "        self.save_interval = save_interval\n",
        "        if not os.path.exists(self.policy_save_dir):\n",
        "            print(\"Directory {} does not exist;\"\n",
        "                  \" creating it now\".format(self.policy_save_dir))\n",
        "            os.makedirs(self.policy_save_dir, exist_ok=True)\n",
        "\n",
        "        if self.use_separate_agents:\n",
        "            self.train_savers = [PolicySaver(self.collect_policies[i],\n",
        "                                             batch_size=None) for i in\n",
        "                                 range(self.num_agents)]\n",
        "            self.eval_savers = [PolicySaver(self.eval_policies[i],\n",
        "                                            batch_size=None) for i in\n",
        "                                range(self.num_agents)]\n",
        "\n",
        "        else:\n",
        "            self.train_saver = PolicySaver(self.collect_policy, batch_size=None)\n",
        "            self.eval_saver = PolicySaver(self.eval_policy, batch_size=None)\n",
        "\n",
        "        self.log_dir = os.path.join(os.path.split(__file__)[0], \"logging\",\n",
        "                                    experiment_name.format(self.time_ext))\n",
        "        self.tb_file_writer = tf.summary.create_file_writer(self.log_dir)\n",
        "        if not os.path.exists(self.log_dir):\n",
        "            os.makedirs(self.log_dir, exist_ok=True)\n",
        "        self.use_tensorboard = use_tensorboard  \n",
        "        self.size = size\n",
        "        self.H, self.W = self.size\n",
        "        self.stacked_channels = self.num_channels * self.num_frames\n",
        "        if self.use_tensorboard:\n",
        "            self.tb_gif_train = np.zeros((self.collect_steps_per_episode,\n",
        "                                         self.num_agents, self.H, self.W,\n",
        "                                          self.stacked_channels))\n",
        "            self.tb_gif_eval = np.zeros((self.max_eval_episode_steps,\n",
        "                                        self.num_agents, self.H, self.W,\n",
        "                                         self.stacked_channels))\n",
        "        # Devices\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        num_gpus = len([x.name for x in local_device_protos if\n",
        "                        x.device_type == 'GPU'])\n",
        "        self.use_gpu = num_gpus > 0\n",
        "\n",
        "    def is_last(self, mode='train'):\n",
        "        \n",
        "        if mode == 'train':\n",
        "            step_types = self.train_env.current_time_step().step_type.numpy()\n",
        "        elif mode == 'eval':\n",
        "            step_types = self.eval_env.current_time_step().step_type.numpy()\n",
        "\n",
        "\n",
        "        is_last = bool(min(np.count_nonzero(step_types == 2), 1))\n",
        "        return is_last\n",
        "\n",
        "    def get_agent_timesteps(self, time_step, step=0,\n",
        "                            only_ego_car=False, ego_car_index=0, max_steps=1000):\n",
        "        \n",
        "        discount = time_step.discount\n",
        "        if len(discount.numpy().shape) > 1 or discount.numpy().shape[0] > 1:\n",
        "            discount = discount[0]\n",
        "        discount = tf.convert_to_tensor(discount, dtype=tf.float32,\n",
        "                                        name='discount')\n",
        "\n",
        "        \n",
        "        if step == 0:  \n",
        "            step_type = 0\n",
        "        elif step == max_steps-1:  \n",
        "            step_type = 2\n",
        "        else:  # Middle time step\n",
        "            step_type = 1\n",
        "        step_type = tf.convert_to_tensor([step_type], dtype=tf.int32,\n",
        "                                         name='step_type')\n",
        "\n",
        "        # Extract rewards for all agents\n",
        "        try:\n",
        "            R = [tf.convert_to_tensor(time_step.reward[:, car_id],\n",
        "                                      dtype=tf.float32, name='reward') \\\n",
        "                 for car_id in range(self.num_agents)]\n",
        "        except:\n",
        "            R = [tf.convert_to_tensor(time_step.reward,\n",
        "                                      dtype=tf.float32, name='reward') \\\n",
        "                 for _ in range(self.num_agents)]\n",
        "\n",
        "        if only_ego_car:\n",
        "            processed_observation = \\\n",
        "                self._process_observations(time_step, only_ego_car=only_ego_car,\n",
        "                                           ego_car_index=ego_car_index)\n",
        "            return ts.TimeStep(step_type, R[ego_car_index], discount,\n",
        "                                   tf.convert_to_tensor(processed_observation,\n",
        "                                   dtype=tf.float32, name='observations'))\n",
        "\n",
        "        else:\n",
        "            processed_observations = \\\n",
        "                self._process_observations(time_step, only_ego_car=only_ego_car,\n",
        "                                           ego_car_index=ego_car_index)\n",
        "            return [ts.TimeStep(step_type, R[car_id], discount,\n",
        "                    tf.convert_to_tensor(processed_observations[car_id],\n",
        "                                         dtype=tf.float32, name='observations'))\n",
        "                    for car_id in range(self.num_agents)]\n",
        "\n",
        "    def _process_observations(self, time_step, only_ego_car=False,\n",
        "                              ego_car_index=0):\n",
        "\n",
        "        if only_ego_car:\n",
        "            input_observation = time_step.observation[:, ego_car_index]\n",
        "\n",
        "            if self.num_agents > 1:  \n",
        "                wrapper = self.observation_wrappers[ego_car_index]\n",
        "            else:\n",
        "                wrapper = self.observation_wrapper\n",
        "\n",
        "            processed_observation = wrapper.get_obs_and_step(input_observation)\n",
        "\n",
        "            return tf.convert_to_tensor(processed_observation, dtype=tf.float32,\n",
        "                                        name='observations')\n",
        "\n",
        "        else:\n",
        "            input_observations = [time_step.observation[:, car_id] for\n",
        "                                  car_id in range(self.num_agents)]\n",
        "            if self.num_agents > 1:  \n",
        "                processed_observations = \\\n",
        "                    [wrapper.get_obs_and_step(input_observation)\n",
        "                     for wrapper, input_observation in\n",
        "                     zip(self.observation_wrappers, input_observations)]\n",
        "\n",
        "            else:  # Single car\n",
        "                processed_observations = \\\n",
        "                    [self.observation_wrapper.get_obs_and_step(\n",
        "                        input_observations[0])]\n",
        "\n",
        "            return [tf.convert_to_tensor(processed_observations[car_id],\n",
        "                                    dtype=tf.float32, name='observations')\n",
        "                    for car_id in range(self.num_agents)]\n",
        "\n",
        "    def collect_step(self, step=0, ego_car_index=0, use_greedy=False,\n",
        "                     add_to_video=False):\n",
        "        \n",
        "        time_step = self.train_env.current_time_step()\n",
        "\n",
        "        actions = []\n",
        "\n",
        "        agent_timesteps = self.get_agent_timesteps(time_step, step=step,\n",
        "                                                   only_ego_car=False,\n",
        "                                                   max_steps=self.max_eval_episode_steps-1)\n",
        "\n",
        "        if self.use_separate_agents:\n",
        "            ego_agent_policy = self.collect_policies[ego_car_index]\n",
        "        else:\n",
        "            ego_agent_policy = self.collect_policy\n",
        "\n",
        "\n",
        "        NUM_AGENTS = 1 \n",
        "        for car_id in range(NUM_AGENTS):\n",
        "\n",
        "            if car_id == ego_car_index:\n",
        "                ego_agent_ts = agent_timesteps[car_id]\n",
        "                ego_action_step = ego_agent_policy.action(ego_agent_ts)\n",
        "                if use_greedy:\n",
        "                    actions.append(ego_action_step.info['loc'])  \n",
        "                else:\n",
        "                    actions.append(ego_action_step.action)  \n",
        "\n",
        "                if self.add_to_video:\n",
        "                    rendered_state = time_step.observation[:, car_id].numpy()\n",
        "                    if self.stacked_channels > 3: \n",
        "                        rendered_state = rendered_state[:, :, :, :3] \n",
        "                    self.video_train.append(rendered_state)\n",
        "\n",
        "            elif self.use_separate_agents:\n",
        "                other_agent_ts = agent_timesteps[car_id]\n",
        "                action_step = self.eval_policies[car_id].action(other_agent_ts)\n",
        "                actions.append(action_step.action)\n",
        "\n",
        "            elif self.use_self_play:\n",
        "                other_agent_ts = agent_timesteps[car_id]\n",
        "                action_step = self.eval_policy.action(other_agent_ts)\n",
        "                actions.append(action_step.action)\n",
        "\n",
        "        if self.use_tensorboard:\n",
        "            processed_observations = self._process_observations(time_step,\n",
        "                                                               only_ego_car=False)\n",
        "            self.tb_gif_train[step] = tf.convert_to_tensor(processed_observations)\n",
        "\n",
        "        action_tensor = tf.convert_to_tensor([tf.stack(tuple(actions), axis=1)])\n",
        "\n",
        "        next_time_step = self.train_env.step(action_tensor)\n",
        "        ego_agent_next_ts = self.get_agent_timesteps(next_time_step, step=step+1,\n",
        "                                                     ego_car_index=ego_car_index,\n",
        "                                                     only_ego_car=True,\n",
        "                                                     max_steps=self.collect_steps_per_episode-1)\n",
        "\n",
        "        traj = trajectory.from_transition(ego_agent_ts, ego_action_step,\n",
        "                                          ego_agent_next_ts)\n",
        "        self.replay_buffer.add_batch(traj)\n",
        "\n",
        "        if add_to_video:\n",
        "            rendered_state = time_step.observation[:, ego_car_index].numpy()\n",
        "            if self.num_frames > 1:  \n",
        "                rendered_state = rendered_state[:, :, :, :3]  # First frame\n",
        "            self.video_train.append(rendered_state)\n",
        "\n",
        "        return float(ego_agent_ts.reward)\n",
        "\n",
        "    def collect_episode(self, epoch=0, ego_car_index=0, add_to_video=False):\n",
        "        \n",
        "        episode_reward = 0  \n",
        "        step = 0  \n",
        "\n",
        "        self.train_env.reset()\n",
        "\n",
        "        use_greedy = float(np.random.binomial(n=1, p=self.epsilon))\n",
        "\n",
        "        while step < self.collect_steps_per_episode and \\\n",
        "                not self.is_last(mode='train'):\n",
        "            episode_reward += self.collect_step(add_to_video=add_to_video,\n",
        "                                                step=step, use_greedy=use_greedy,\n",
        "                                                ego_car_index=ego_car_index)\n",
        "            step += 1\n",
        "\n",
        "        self.global_step += step\n",
        "\n",
        "        if self.use_tensorboard:\n",
        "            with self.tb_file_writer.as_default():\n",
        "                tf.summary.scalar(\"Average Training Reward\", float(episode_reward),\n",
        "                                  step=self.global_step)\n",
        "                frames = self.tb_gif_train\n",
        "                video_summary(\"train/grid\", frames, fps=self.FPS,\n",
        "                              step=self.global_step, channels=self.num_channels)\n",
        "\n",
        "            self.tb_gif_train = np.zeros((self.collect_steps_per_episode,\n",
        "                                         self.num_agents, self.H, self.W,\n",
        "                                          self.stacked_channels))\n",
        "\n",
        "    def compute_average_reward(self, ego_car_index=0):\n",
        "        \n",
        "        total_return = 0.0  \n",
        "\n",
        "        for e in range(self.num_eval_episodes):\n",
        "            time_step = self.eval_env.reset()\n",
        "\n",
        "            # Initialize step counter and episode_return\n",
        "            i = 0\n",
        "            episode_return = 0.0\n",
        "            while i < self.max_eval_episode_steps and \\\n",
        "                    not self.is_last(mode='eval'):\n",
        "                actions = []\n",
        "\n",
        "                agent_timesteps = self.get_agent_timesteps(time_step, step=i)\n",
        "                for car_id in range(self.num_agents):\n",
        "\n",
        "                    if car_id == ego_car_index:  \n",
        "                        ego_agent_ts = agent_timesteps[car_id]\n",
        "                        rendered_state = ego_agent_ts.observation.numpy()\n",
        "                        if self.num_frames > 1:  \n",
        "                            rendered_state = rendered_state[..., :3] \n",
        "                        self.video_eval.append(rendered_state)\n",
        "\n",
        "                        if self.use_separate_agents:  \n",
        "                            ego_action_step = self.eval_policies[car_id].action(ego_agent_ts)\n",
        "                            actions.append(ego_action_step.action)   \n",
        "\n",
        "                        elif self.use_self_play:  \n",
        "                            ego_action_step = self.eval_policy.action(ego_agent_ts)\n",
        "                            actions.append(ego_action_step.action) \n",
        "\n",
        "                    elif self.use_separate_agents:\n",
        "                        other_agent_ts = agent_timesteps[car_id]\n",
        "                        action_step = self.eval_policies[car_id].action(other_agent_ts)\n",
        "                        actions.append(action_step.action)\n",
        "\n",
        "                   \n",
        "                    elif self.use_self_play:\n",
        "                        other_agent_ts = agent_timesteps[car_id]\n",
        "                        action_step = self.eval_policy.action(other_agent_ts)\n",
        "                        actions.append(action_step.action)\n",
        "\n",
        "                action_tensor = tf.convert_to_tensor([tf.stack(tuple(actions),\n",
        "                                                               axis=1)])\n",
        "\n",
        "                time_step = self.eval_env.step(action_tensor)\n",
        "\n",
        "                if self.use_tensorboard:\n",
        "                    processed_observations = self._process_observations(time_step,\n",
        "                                                                        only_ego_car=False)\n",
        "                    self.tb_gif_eval[i] = tf.convert_to_tensor(processed_observations)\n",
        "\n",
        "                episode_return += ego_agent_ts.reward  \n",
        "                if i % 250 == 0:\n",
        "                    action = ego_action_step.action.numpy()\n",
        "                    print(\"Action: {}, \"\n",
        "                          \"Reward: {}\".format(action, episode_return))\n",
        "                i += 1\n",
        "\n",
        "            print(\"Steps in episode: {}\".format(i))\n",
        "            total_return += episode_return\n",
        "        avg_return = total_return / self.num_eval_episodes\n",
        "\n",
        "        if self.use_tensorboard:\n",
        "            with self.tb_file_writer.as_default():\n",
        "                video_summary(\"eval/grid\".format(car_id), self.tb_gif_eval,\n",
        "                              fps=self.FPS, step=self.global_step,\n",
        "                              channels=self.num_channels)\n",
        "\n",
        "            self.tb_gif_eval = np.zeros((self.max_eval_episode_steps,\n",
        "                                        self.num_agents, self.H, self.W,\n",
        "                                         self.stacked_channels))\n",
        "\n",
        "        print(\"Average return: {}\".format(avg_return))\n",
        "\n",
        "\n",
        "        if self.use_separate_agents:  \n",
        "            self.eval_returns[ego_car_index].append(avg_return)\n",
        "        else:  \n",
        "            self.eval_returns.append(avg_return)\n",
        "\n",
        "        return avg_return\n",
        "\n",
        "    def collect_step_lstm(self, step=0, ego_car_index=0, add_to_video=False,\n",
        "                          policy_states=None):\n",
        "        \n",
        "        time_step = self.train_env.current_time_step()\n",
        "\n",
        "        # Create empty list of actions and next policy states\n",
        "        actions = []\n",
        "        next_policy_states = {}\n",
        "\n",
        "        agent_timesteps = self.get_agent_timesteps(time_step, step=step,\n",
        "                                                   only_ego_car=False)\n",
        "\n",
        "        if self.use_separate_agents:\n",
        "            ego_agent_policy = self.collect_policies[ego_car_index]\n",
        "        else:\n",
        "            ego_agent_policy = self.collect_policy\n",
        "\n",
        "        for car_id in range(self.num_agents):\n",
        "            if car_id == ego_car_index:\n",
        "                ego_agent_ts = agent_timesteps[car_id]\n",
        "                if self.use_separate_agents:\n",
        "                    ego_policy_step = ego_agent_policy.action(\n",
        "                        ego_agent_ts, policy_states[car_id])\n",
        "                else:\n",
        "                    ego_policy_step = self.collect_policy.action(ego_agent_ts,\n",
        "                                                                 policy_states[car_id])\n",
        "                if use_greedy:\n",
        "                    actions.append(ego_action_step.info['loc']) \n",
        "                else:\n",
        "                    actions.append(ego_action_step.action)  \n",
        "                policy_state = ego_policy_step.state\n",
        "\n",
        "                if self.add_to_video:\n",
        "                    rendered_state = time_step.observation[:, car_id].numpy()\n",
        "                    if self.num_frames > 1:  \n",
        "                        rendered_state = rendered_state[..., :3] \n",
        "                    self.video_train.append(rendered_state)\n",
        "\n",
        "            elif self.use_separate_agents:\n",
        "                other_agent_ts = agent_timesteps[car_id]\n",
        "                policy_step = self.eval_policies[car_id].action(other_agent_ts, policy_states[car_id])\n",
        "                policy_state = policy_step.state\n",
        "                actions.append(policy_step.action)  \n",
        "\n",
        "            elif self.use_self_play:\n",
        "                other_agent_ts = agent_timesteps[car_id]\n",
        "                policy_step = self.eval_policy.action(other_agent_ts, policy_states[car_id])\n",
        "                policy_state = policy_step.state\n",
        "                actions.append(policy_step.action)\n",
        "\n",
        "            next_policy_states[car_id] = policy_state  \n",
        "\n",
        "        if self.use_tensorboard:\n",
        "            processed_observations = self._process_observations(time_step,\n",
        "                                                                only_ego_car=False)\n",
        "            self.tb_gif_train[step] = tf.convert_to_tensor(processed_observations)\n",
        "\n",
        "        action_tensor = tf.convert_to_tensor([tf.stack(tuple(actions), axis=1)])\n",
        "\n",
        "        next_time_step = self.train_env.step(action_tensor)\n",
        "        ego_agent_next_ts = self.get_agent_timesteps(next_time_step,\n",
        "                                                     step=step + 1,\n",
        "                                                     ego_car_index=ego_car_index,\n",
        "                                                     only_ego_car=True)\n",
        "\n",
        "        traj = trajectory.from_transition(ego_agent_ts, ego_policy_step,\n",
        "                                          ego_agent_next_ts)\n",
        "        self.replay_buffer.add_batch(traj)\n",
        "\n",
        "        if add_to_video:\n",
        "            rendered_state = time_step.observation[:, ego_car_index].numpy()\n",
        "            if self.num_frames > 1:\n",
        "                rendered_state = rendered_state[:, :, :, 3]  \n",
        "            self.video_train.append(rendered_state)\n",
        "\n",
        "        return next_policy_states, float(ego_agent_ts.reward)\n",
        "\n",
        "    def reset_policy_states(self, ego_car_index=0, mode='train'):\n",
        "        \n",
        "        if mode == 'train':\n",
        "            if self.use_separate_agents:\n",
        "                policy_states = {car_id: self.eval_policies[car_id].get_initial_state(\n",
        "                    self.train_env.batch_size) for car_id in range(self.num_agents)}\n",
        "                policy_states[ego_car_index] = self.collect_policies[\n",
        "                    ego_car_index].get_initial_state(self.train_env.batch_size)\n",
        "            else:\n",
        "                policy_states = {car_id: self.eval_policy.get_initial_state(self.train_env.batch_size)\n",
        "                                 for car_id in range(self.num_agents)}\n",
        "                policy_states[ego_car_index] = self.collect_policy.get_initial_state(\n",
        "                    self.train_env.batch_size)\n",
        "\n",
        "        elif mode == 'eval':\n",
        "            if self.use_separate_agents:\n",
        "                policy_states = {\n",
        "                    car_id: self.eval_policies[car_id].get_initial_state(\n",
        "                        self.eval_env.batch_size) for car_id in\n",
        "                    range(self.num_agents)}\n",
        "            else:\n",
        "                policy_states = {car_id: self.eval_policy.get_initial_state(\n",
        "                    self.eval_env.batch_size) for car_id in\n",
        "                    range(self.num_agents)}\n",
        "\n",
        "        return policy_states\n",
        "\n",
        "    def collect_episode_lstm(self, epoch=0, ego_car_index=0, add_to_video=False):\n",
        "        \n",
        "        policy_states = self.reset_policy_states(ego_car_index=ego_car_index)\n",
        "\n",
        "        episode_reward = 0  \n",
        "        step = 0\n",
        "\n",
        "        self.train_env.reset()\n",
        "\n",
        "        use_greedy = float(np.random.binomial(n=1, p=self.epsilon))\n",
        "\n",
        "        while step < self.collect_steps_per_episode and \\\n",
        "                not self.is_last(mode='train'):\n",
        "            if step % 1000 == 0:\n",
        "                print(\"Step number: {}\".format(step))\n",
        "            policy_states, ego_reward = self.collect_step_lstm(add_to_video=add_to_video,\n",
        "                                                               step=step, use_greedy=use_greedy,\n",
        "                                                               ego_car_index=ego_car_index,\n",
        "                                                               policy_states=policy_states)\n",
        "            episode_reward += ego_reward\n",
        "            step += 1\n",
        "\n",
        "        self.global_step += step\n",
        "\n",
        "        if self.use_tensorboard:\n",
        "            with self.tb_file_writer.as_default():\n",
        "                tf.summary.scalar(\"Average Training Reward\", float(episode_reward),\n",
        "                                  step=self.global_step)\n",
        "                frames = self.tb_gif_train\n",
        "                video_summary(\"train/grid\", frames, fps=self.FPS,\n",
        "                              step=self.global_step,\n",
        "                              channels=self.num_channels)\n",
        "\n",
        "            self.tb_gif_train = np.zeros((self.collect_steps_per_episode,\n",
        "                                          self.num_agents, self.H, self.W,\n",
        "                                          self.stacked_channels))\n",
        "\n",
        "    def compute_average_reward_lstm(self, ego_car_index=0):\n",
        "        \n",
        "        total_return = 0.0\n",
        "\n",
        "        for _ in range(self.num_eval_episodes):\n",
        "            time_step = self.eval_env.reset()\n",
        "\n",
        "            i = 0\n",
        "            episode_return = 0.0\n",
        "\n",
        "            policy_states = self.reset_policy_states(ego_car_index=ego_car_index,\n",
        "                                                     mode='eval')\n",
        "\n",
        "            while i < self.max_eval_episode_steps and \\\n",
        "                    not self.is_last(mode='eval'):\n",
        "\n",
        "                actions = []\n",
        "\n",
        "                agent_timesteps = self.get_agent_timesteps(time_step, step=i)\n",
        "                NUM_AGENTS = 1\n",
        "                for car_id in range(NUM_AGENTS):\n",
        "\n",
        "                    if car_id == ego_car_index:\n",
        "                        ego_agent_ts = agent_timesteps[car_id]\n",
        "                        rendered_state = ego_agent_ts.observation.numpy()\n",
        "                        if self.num_frames > 1:\n",
        "                            rendered_state = rendered_state[..., :3]  \n",
        "                        self.video_eval.append(rendered_state)\n",
        "\n",
        "                        if self.use_separate_agents:  \n",
        "                            ego_policy_step = self.eval_policies[car_id].action(ego_agent_ts,\n",
        "                                                                                policy_states[car_id])\n",
        "                            actions.append(ego_policy_step.action)  \n",
        "\n",
        "                        elif self.use_self_play:  \n",
        "                            ego_policy_step = self.eval_policy.action(ego_agent_ts,\n",
        "                                                                      policy_states[car_id])\n",
        "                            actions.append(ego_policy_step.action)  \n",
        "\n",
        "                        policy_state = ego_policy_step.state\n",
        "\n",
        "                    elif self.use_separate_agents:\n",
        "                        other_agent_ts = agent_timesteps[car_id]\n",
        "                        policy_step = self.eval_policies[car_id].action(other_agent_ts,\n",
        "                                                                        policy_states[car_id])\n",
        "                        actions.append(policy_step.action)\n",
        "                        policy_state = policy_step.state\n",
        "\n",
        "                    elif self.use_self_play:\n",
        "                        other_agent_ts = agent_timesteps[car_id]\n",
        "                        policy_step = self.eval_policy.action(other_agent_ts,\n",
        "                                                              policy_states[car_id])\n",
        "                        actions.append(policy_step.action)\n",
        "                        policy_state = policy_step.state\n",
        "\n",
        "                    policy_states[car_id] = policy_state\n",
        "\n",
        "                action_tensor = tf.convert_to_tensor([tf.stack(tuple(actions), axis=1)])\n",
        "\n",
        "                time_step = self.eval_env.step(action_tensor)\n",
        "\n",
        "                if self.use_tensorboard:\n",
        "                    processed_observations = self._process_observations(time_step,\n",
        "                                                                        only_ego_car=False)\n",
        "                    self.tb_gif_eval[i] = tf.convert_to_tensor(processed_observations)\n",
        "\n",
        "                episode_return += ego_agent_ts.reward \n",
        "                if i % 250 == 0:\n",
        "                    action = ego_policy_step.action.numpy()\n",
        "                    print(\"Action: {}, \"\n",
        "                          \"Reward: {}\".format(action, episode_return))\n",
        "                    print(\"POLICY STATES: {}\".format(\n",
        "                        [np.sum(policy_states[i]) for i\n",
        "                         in range(self.num_agents)]))\n",
        "                i += 1\n",
        "            print(\"Steps in episode: {}\".format(i))\n",
        "            total_return += episode_return\n",
        "        avg_return = total_return / self.num_eval_episodes\n",
        "\n",
        "        \n",
        "        print(\"Average return: {}\".format(avg_return))\n",
        "\n",
        "        if self.use_separate_agents:  \n",
        "            self.eval_returns[ego_car_index].append(avg_return)\n",
        "        else: \n",
        "            self.eval_returns.append(avg_return)\n",
        "\n",
        "        return avg_return\n",
        "\n",
        "    def train_agent(self):\n",
        "        \n",
        "        eval_epochs = []\n",
        "\n",
        "        # Optimize by wrapping some of the code in a graph using TF function.\n",
        "        if self.use_separate_agents:\n",
        "            for car_id in range(self.num_agents):\n",
        "                self.agents[car_id].train = common.function(self.agents[car_id].train)\n",
        "                self.agents[car_id].train_step_counter.assign(0)\n",
        "        else:\n",
        "            self.agent.train = common.function(self.agent.train)\n",
        "\n",
        "        # Compute pre-training returns\n",
        "        if self.use_lstm:\n",
        "            avg_return = self.compute_average_reward_lstm(ego_car_index=0)\n",
        "\n",
        "        else:\n",
        "            avg_return = self.compute_average_reward(ego_car_index=0)\n",
        "\n",
        "        # Log average training return to tensorboard\n",
        "        if self.use_tensorboard:\n",
        "            with self.tb_file_writer.as_default():\n",
        "                tf.summary.scalar(\"Average Eval Reward\", float(avg_return),\n",
        "                                  step=self.global_step)\n",
        "\n",
        "        print(\"DONE WITH PRELIMINARY EVALUATION...\")\n",
        "\n",
        "        # Append for output plot, create video, and empty eval video array\n",
        "        eval_epochs.append(0)\n",
        "        self.create_video(mode='eval', ext=0)\n",
        "        self.video_eval = []  # Empty to create a new eval video\n",
        "        returns = [avg_return]\n",
        "\n",
        "        # Reset the environment time step and global and episode step counters\n",
        "#        time_step = self.train_env.reset()\n",
        "        step = 0\n",
        "        i = 0\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            if self.global_step >= self.total_steps:\n",
        "                print(\"Reached the end of training with {} training steps\".format(self.global_step))\n",
        "                break\n",
        "\n",
        "            ego_car_index = i % self.num_agents\n",
        "            print(\"Training epoch: {}\".format(i))\n",
        "\n",
        "            print(\"Collecting episode for car with ID {}\".format(ego_car_index))\n",
        "\n",
        "            self.video_train = []\n",
        "\n",
        "            if self.use_lstm:\n",
        "                self.collect_episode_lstm(epoch=i, ego_car_index=ego_car_index)\n",
        "                print(\"LSTM\")\n",
        "\n",
        "            else:\n",
        "                self.collect_episode(epoch=i, ego_car_index=ego_car_index)\n",
        "                print(\"No LSTM\")\n",
        "\n",
        "            if i % 100 == 0 and self.add_to_video:\n",
        "                self.create_video(mode='train', ext=i)  \n",
        "            print(\"Collected Episode\")\n",
        "\n",
        "            if self.use_gpu:\n",
        "                device = '/gpu:0'\n",
        "            else:\n",
        "                device = '/cpu:0'\n",
        "\n",
        "            with tf.device(device):\n",
        "\n",
        "                trajectories = self.replay_buffer.gather_all()\n",
        "\n",
        "                if self.use_separate_agents:\n",
        "\n",
        "                    train_loss = self.agents[ego_car_index].train(experience=trajectories)\n",
        "\n",
        "                    if self.use_tensorboard:\n",
        "                        with self.tb_file_writer.as_default():\n",
        "                            tf.summary.scalar(\"Training Loss Agent {}\".format(ego_car_index),\n",
        "                                              float(train_loss.loss),\n",
        "                                              step=self.global_step // self.num_agents)\n",
        "\n",
        "                    step = self.agents[ego_car_index].train_step_counter.numpy()\n",
        "\n",
        "\n",
        "                else:\n",
        "                    train_loss = self.agent.train(experience=trajectories)\n",
        "\n",
        "                    if self.use_tensorboard:\n",
        "                        with self.tb_file_writer.as_default():\n",
        "                            tf.summary.scalar(\"Training Loss\",\n",
        "                                              float(train_loss.loss), step=self.global_step)\n",
        "\n",
        "            with tf.device('/cpu:0'):\n",
        "\n",
        "                if self.global_step % self.log_interval == 0:\n",
        "                    print('step = {0}: loss = {1}'.format(self.global_step,\n",
        "                                                          train_loss.loss))\n",
        "\n",
        "                if i % self.eval_interval == 0:\n",
        "\n",
        "                    if self.use_lstm:\n",
        "                        avg_return = self.compute_average_reward_lstm(ego_car_index=ego_car_index)\n",
        "                    else:\n",
        "                        avg_return = self.compute_average_reward(ego_car_index=ego_car_index)\n",
        "\n",
        "                    if self.use_tensorboard:\n",
        "                        with self.tb_file_writer.as_default():\n",
        "                            tf.summary.scalar(\"Average Eval Reward\",\n",
        "                                              float(avg_return),\n",
        "                                              step=self.global_step)\n",
        "                    eval_epochs.append(i + 1)\n",
        "                    print(\n",
        "                        'epoch = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "                    returns.append(avg_return)\n",
        "                    if self.add_to_video:\n",
        "                        self.create_video(mode='eval', ext=i)\n",
        "                    self.video_eval = []  \n",
        "\n",
        "                if i % self.save_interval == 0 and i != 0:\n",
        "                    self.save_policies(epochs_done=i)\n",
        "                    print(\"Epochs: {}\".format(i))\n",
        "\n",
        "                self.replay_buffer.clear()\n",
        "\n",
        "        if self.use_separate_agents:\n",
        "            return self.agents\n",
        "        else:\n",
        "            return self.agent\n",
        "\n",
        "    def create_video(self, mode='eval', ext=0, ego_car_index=0):\n",
        "        \n",
        "        if mode == 'eval':  \n",
        "            video = self.video_eval\n",
        "        elif mode == 'train':  \n",
        "            video = self.video_train\n",
        "\n",
        "        if len(video) == 0:\n",
        "            raise AssertionError(\"Video is empty.\")\n",
        "        print(\"Number of frames in video: {}\".format(len(video)))\n",
        "        obs_size = video[0].shape\n",
        "        width = np.uint(obs_size[-3])\n",
        "        height = np.uint(obs_size[-2])\n",
        "        channels = np.uint(obs_size[-1])\n",
        "        print(\"HEIGHT IS: {}, WIDTH IS: {}, CHANNELS IS: {}\".format(width, height, channels))\n",
        "\n",
        "        fourcc = cv.VideoWriter_fourcc(*'XVID')\n",
        "        out_file = os.path.join(self.log_dir,\n",
        "                                \"trajectories_{}_epoch_{}_agent_{}\"\n",
        "                                \".avi\".format(mode, ext, ego_car_index))\n",
        "        out = cv.VideoWriter(out_file, fourcc, self.FPS, (width, height))\n",
        "\n",
        "        for i in range(len(video)):\n",
        "            img_rgb = cv.cvtColor(np.uint8(255 * video[i][0]),\n",
        "                                  cv.COLOR_BGR2RGB)  # Save as RGB image\n",
        "            out.write(img_rgb)\n",
        "        out.release()\n",
        "\n",
        "    def plot_eval(self):\n",
        "    \n",
        "        if self.use_separate_agents:  \n",
        "            for car_id in range(self.num_agents):\n",
        "                xs = [i * self.eval_interval for\n",
        "                      i in range(len(self.eval_returns[car_id]))]\n",
        "                plt.plot(xs, self.eval_returns[car_id])\n",
        "                plt.xlabel(\"Training epochs\")\n",
        "                plt.ylabel(\"Average Return\")\n",
        "                plt.title(\"Average Returns as a Function \"\n",
        "                          \"of Training (Agent {})\".format(car_id))\n",
        "                save_path = os.path.join(self.policy_save_dir,\n",
        "                                         \"eval_returns_agent_{}\"\n",
        "                                         \".png\".format(car_id))\n",
        "                plt.savefig(save_path)\n",
        "                print(\"Created plot of returns for agent {}...\".format(car_id))\n",
        "\n",
        "        else:\n",
        "            xs = [i * self.eval_interval for i in range(len(self.eval_returns))]\n",
        "            plt.plot(xs, self.eval_returns)\n",
        "            plt.xlabel(\"Training epochs\")\n",
        "            plt.ylabel(\"Average Return\")\n",
        "            plt.title(\"Average Returns as a Function of Training\")\n",
        "            save_path = os.path.join(self.policy_save_dir, \"eval_returns.png\")\n",
        "            plt.savefig(save_path)\n",
        "            print(\"CREATED PLOT OF RETURNS\")\n",
        "\n",
        "\n",
        "    def save_policies(self, epochs_done=0, is_final=False):\n",
        "       \n",
        "        if is_final:\n",
        "            epochs_done = \"FINAL\"\n",
        "\n",
        "        if self.use_separate_agents:\n",
        "\n",
        "            for i, train_saver in enumerate(self.train_savers):\n",
        "                if custom_path is None:\n",
        "                    train_save_dir = os.path.join(self.policy_save_dir, \"train\",\n",
        "                                                 \"epochs_{}\".format(epochs_done),\n",
        "                                                 \"agent_{}\".format(i))\n",
        "                else:\n",
        "                    train_save_dir = os.path.join(self.policy_save_dir, \"train\",\n",
        "                                                  \"epochs_{}\".format(\n",
        "                                                      custom_path),\n",
        "                                                  \"agent_{}\".format(i))\n",
        "                if not os.path.exists(train_save_dir):\n",
        "                    os.makedirs(train_save_dir, exist_ok=True)\n",
        "                train_saver.save(train_save_dir)\n",
        "\n",
        "            print(\"Training policies saved...\")\n",
        "\n",
        "            for i, eval_saver in enumerate(self.eval_savers):\n",
        "                eval_save_dir = os.path.join(self.policy_save_dir, \"eval\",\n",
        "                                             \"epochs_{}\".format(epochs_done),\n",
        "                                             \"agent_{}\".format(i))\n",
        "                if not os.path.exists(eval_save_dir):\n",
        "                    os.makedirs(eval_save_dir, exist_ok=True)\n",
        "                eval_saver.save(eval_save_dir)\n",
        "\n",
        "            print(\"Eval policies saved...\")\n",
        "\n",
        "        else:\n",
        "            train_save_dir = os.path.join(self.policy_save_dir, \"train\",\n",
        "                                          \"epochs_{}\".format(epochs_done))\n",
        "            if not os.path.exists(train_save_dir):\n",
        "                os.makedirs(train_save_dir, exist_ok=True)\n",
        "            self.train_saver.save(train_save_dir)\n",
        "\n",
        "            print(\"Training policy saved...\")\n",
        "\n",
        "            # Save eval policy\n",
        "            eval_save_dir = os.path.join(self.policy_save_dir, \"eval\",\n",
        "                                         \"epochs_{}\".format(epochs_done))\n",
        "            if not os.path.exists(eval_save_dir):\n",
        "                os.makedirs(eval_save_dir, exist_ok=True)\n",
        "            self.eval_saver.save(eval_save_dir)\n",
        "\n",
        "            print(\"Eval policy saved...\")\n",
        "\n",
        "        agent_params = {'normalize_obs': self.train_env.normalize,\n",
        "                        'use_lstm': self.use_lstm,\n",
        "                        'frame_stack': self.use_multiple_frames,\n",
        "                        'num_frame_stack': self.env.num_frame_stack,\n",
        "                        'obs_size': self.size}\n",
        "\n",
        "        params_path = os.path.join(self.policy_save_dir, \"parameters.pkl\")\n",
        "        with open(params_path, \"w\") as pkl_file:\n",
        "            pickle.dump(agent_params, pkl_file)\n",
        "        pkl_file.close()\n",
        "\n",
        "    def load_saved_policies(self, eval_model_path=None, train_model_path=None):\n",
        "    \n",
        "        if eval_model_path is not None:\n",
        "            self.eval_policy = tf.saved_model.load(eval_model_path)\n",
        "            print(\"Loading evaluation policy from: {}\".format(eval_model_path))\n",
        "\n",
        "        if train_model_path is not None:\n",
        "            self.collect_policy = tf.saved_model.load(train_model_path)\n",
        "            print(\"Loading training policy from: {}\".format(train_model_path))\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    \"\"\"Argument-parsing function for running this code.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"-n\", \"--num_agents\", default=2, type=int,\n",
        "                        help=\"Number of cars in the environment.\")\n",
        "    parser.add_argument(\"-size\", \"--size\", required=False,\n",
        "                        default=\"96\",\n",
        "                        help=\"The width and height of the observation window.\")\n",
        "    parser.add_argument(\"-direction\", \"--direction\", type=str, default='CCW',\n",
        "                        help=\"Direction in which agents traverse the track.\")\n",
        "    parser.add_argument(\"-random_direction\", \"--use_random_direction\",\n",
        "                        required=False, action='store_true',\n",
        "                        help=\"Whether agents are trained/evaluated on \"\n",
        "                             \"both CW and CCW trajectories across the track.\")\n",
        "    parser.add_argument(\"-backwards_flag\", \"--backwards_flag\", required=False,\n",
        "                        action=\"store_true\",\n",
        "                        help=\"Whether to render a backwards flag indicator when \"\n",
        "                             \"an agent drives on the track backwards.\")\n",
        "    parser.add_argument(\"-h_ratio\", \"--h_ratio\", type=float, default=0.25,\n",
        "                        help=\"Default height location fraction for where car\"\n",
        "                             \"is located in observation upon rendering.\")\n",
        "    parser.add_argument(\"-ego_color\", \"--use_ego_color\", required=False,\n",
        "                        action=\"store_true\", default=\"Whether to render each \"\n",
        "                                                     \"ego car in the same color.\")\n",
        "\n",
        "    parser.add_argument(\"-self_play\", \"--use_self_play\",\n",
        "                        required=False, action=\"store_true\",\n",
        "                        help=\"Flag for whether to use a single master PPO agent.\")\n",
        "    parser.add_argument(\"-n_agents\", \"--use_separate_agents\",\n",
        "                        required=False, action=\"store_true\",\n",
        "                        help=\"Flag for whether to use a N PPO agents.\")\n",
        "\n",
        "    parser.add_argument(\"-epochs\", \"--total_epochs\", default=1000, type=int,\n",
        "                        help=\"Number of epochs to train agent over.\")\n",
        "    parser.add_argument(\"-steps\", \"--total_steps\", type=int, default=10e6,\n",
        "                        help=\"Total number of training steps to take.\")\n",
        "    parser.add_argument(\"-collect_episode_steps\", \"--collect_steps_per_episode\",\n",
        "                        default=1000, type=int,\n",
        "                        help=\"Number of steps to take per collection episode.\")\n",
        "    parser.add_argument(\"-eval_episode_steps\", \"--eval_steps_per_episode\",\n",
        "                        default=1000, type=int,\n",
        "                        help=\"Number of steps to take per evaluation episode.\")\n",
        "    parser.add_argument(\"-eval_interval\", \"--eval_interval\", default=10,\n",
        "                        type=int,\n",
        "                        help=\"Evaluate every time epoch % eval_interval = 0.\")\n",
        "    parser.add_argument(\"-eval_episodes\", \"--num_eval_episodes\", default=5,\n",
        "                        type=int,\n",
        "                        help=\"Evaluate over eval_episodes evaluation episodes.\")\n",
        "    parser.add_argument(\"-lr\", \"--learning_rate\", default=5e-8, type=float,\n",
        "                        help=\"Learning rate for PPO agent(s).\")\n",
        "    parser.add_argument(\"-lstm\", \"--use_lstm\", required=False, action=\"store_true\",\n",
        "                        help=\"Flag for whether to use LSTMs on actor and critic\"\n",
        "                             \"networks of the PPO agent.\")\n",
        "    parser.add_argument(\"-eps\", \"--epsilon\", type=float, default=0.0,\n",
        "                        help=\"Probability of training on the greedy policy for a\"\n",
        "                             \"given episode\")\n",
        "\n",
        "    parser.add_argument(\"-si\", \"--save_interval\", default=10, type=int,\n",
        "                        help=\"Save policies every time epoch % eval_interval = 0.\")\n",
        "    parser.add_argument(\"-li\", \"--log_interval\", default=1, type=int,\n",
        "                        help=\"Log results every time epoch % eval_interval = 0.\")\n",
        "    parser.add_argument(\"-tb\", \"--use_tensorboard\", required=False,\n",
        "                        action=\"store_true\", help=\"Log with tensorboard as well.\")\n",
        "    parser.add_argument(\"-add_to_video\", \"--add_to_video\", required=False,\n",
        "                        action=\"store_true\",\n",
        "                        help=\"Whether to save trajectories as videos.\")\n",
        "\n",
        "    parser.add_argument(\"-exp_name\", \"--experiment_name\", type=str,\n",
        "                        default=\"experiment_{}\", required=False,\n",
        "                        help=\"Name of experiment (for logging).\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"Your selected training parameters: \\n {}\".format(vars(args)))\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    main()"
      ],
      "metadata": {
        "id": "1xwCOKKnwpFw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXYWtk4MLDTI"
      },
      "source": [
        "# КОД из ноутбука Optima_1\n",
        "@author: Vadim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install control"
      ],
      "metadata": {
        "id": "JowWYy9k6nqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# optimal_bench.py - benchmarks for optimal control package\n",
        "# RMM, 27 Feb 2020\n",
        "#\n",
        "# This benchmark tests the timing for the optimal control module\n",
        "# (control.optimal) and is intended to be used for helping tune the\n",
        "# performance of the functions used for optimization-base control.\n",
        "\n",
        "import math\n",
        "import control as ct\n",
        "import control.flatsys as flat\n",
        "import control.optimal as opt\n",
        "import logging\n",
        "\n",
        "\n",
        "#\n",
        "# Vehicle steering dynamics\n",
        "#\n",
        "# The vehicle dynamics are given by a simple bicycle model.  We take the state\n",
        "# of the system as (x, y, theta) where (x, y) is the position of the vehicle\n",
        "# in the plane and theta is the angle of the vehicle with respect to\n",
        "# horizontal.  The vehicle input is given by (v, phi) where v is the forward\n",
        "# velocity of the vehicle and phi is the angle of the steering wheel.  The\n",
        "# model includes saturation of the vehicle steering angle.\n",
        "#\n",
        "# System state: x, y, theta\n",
        "# System input: v, phi\n",
        "# System output: x, y\n",
        "# System parameters: wheelbase, maxsteer\n",
        "#\n",
        "def vehicle_update(t, x, u, params):\n",
        "    # Get the parameters for the model\n",
        "    l = params.get('wheelbase', 3.)         # vehicle wheelbase\n",
        "    phimax = params.get('maxsteer', 0.5)    # max steering angle (rad)\n",
        "    # Saturate the steering input (use min/max instead of clip for speed)\n",
        "    phi = max(-phimax, min(u[1], phimax))\n",
        "    # Return the derivative of the state\n",
        "    return np.array([\n",
        "        math.cos(x[2]) * u[0],            # xdot = cos(theta) v\n",
        "        math.sin(x[2]) * u[0],            # ydot = sin(theta) v\n",
        "        (u[0] / l) * math.tan(phi)        # thdot = v/l tan(phi)\n",
        "    ])\n",
        "\n",
        "\n",
        "def vehicle_output(t, x, u, params):\n",
        "    return x                            # return x, y, theta (full state)\n",
        "\n",
        "vehicle = ct.NonlinearIOSystem(\n",
        "    vehicle_update, vehicle_output, states=3, name='vehicle',\n",
        "    inputs=('v', 'phi'), outputs=('x', 'y', 'theta'))\n",
        "\n",
        "# Initial and final conditions\n",
        "x0 = [0., -2., 0.]; u0 = [10., 0.] #начальное положение автомобиля\n",
        "xf = [120., 1.1, 0.]; uf = [10., 0.] #конечное положение автомобиля\n",
        "Tf = 12.14 # увеличение значения приводит к изменению траектории (см.первый график)\n",
        "# траектория укорачивается при уменьшении значения Tf\n",
        "\n",
        "# Define the time horizon (and spacing) for the optimization\n",
        "horizon = np.linspace(0, Tf, 10, endpoint=True)\n",
        "# Provide an intial guess (will be extended to entire horizon)\n",
        "bend_left = [10.9405, 0.01]          # slight left veer\n",
        "\n",
        "# Set up the cost functions\n",
        "Q = np.diag([5, 10, 1])     # keep lateral error low\n",
        "R = np.diag([.1, 1])          # minimize applied inputs\n",
        "quad_cost = opt.quadratic_cost(vehicle, Q, R, x0=xf, u0=uf)\n",
        "#        \n",
        "res = opt.solve_ocp(\n",
        "        vehicle, horizon, x0, quad_cost,\n",
        "        initial_guess=bend_left, print_summary=False,\n",
        "        # solve_ivp_kwargs={'atol': 1e-2, 'rtol': 1e-2},\n",
        "        minimize_method='trust-constr',\n",
        "        minimize_options={'finite_diff_rel_step': 0.01},\n",
        "    )\n",
        "# функция u зависящая от угол поворота и скорость\n",
        "# выходные параметры: время, координата y\n",
        "u = res.inputs\n",
        "t, y = ct.input_output_response(vehicle, horizon, u, x0) \n",
        "#y = res.states\n",
        "#t = res.time\n",
        "\n",
        "print('Время t:\\n', t)\n",
        "print('Функция g(х,y):\\n', y)\n",
        "print('Функция u(скорость, угол):\\n', u)\n",
        "\n",
        "plt.figure(figsize=(10, 10)) # увеличение размера графиков\n",
        "\n",
        "# Plot the results\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(y[0], y[1])\n",
        "plt.plot(x0[0], x0[1], 'ro', xf[0], xf[1], 'ro')\n",
        "plt.xlabel(\"Координата x [m]\")\n",
        "plt.ylabel(\"Координата y [m]\")\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(t, u[0])\n",
        "plt.axis([0, 10, 5.5, 22.5])\n",
        "plt.plot([0, 10], [9, 9], 'k--', [0, 10], [11, 11], 'k--') # установка диапазона значений\n",
        "plt.xlabel(\"Время t [sec]\")\n",
        "plt.ylabel(\"Скорость u1 [m/s]\")\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(t, u[1])\n",
        "plt.axis([0, 10, -0.35, 0.35])\n",
        "plt.plot([0, 10], [-0.1, -0.1], 'k--', [0, 10], [0.1, 0.1], 'k--') # установка диапазона значений\n",
        "plt.xlabel(\"Время t [sec]\")\n",
        "plt.ylabel(\"Угловая скорость u2 [rad/s]\")\n",
        "\n",
        "plt.suptitle(\"Траектория движения\") #Lane change manuever\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ady0byiAx-pi",
        "outputId": "218b36c2-d4d5-42c6-8aac-429a44aefd51"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary statistics:\n",
            "* Cost function calls: 693\n",
            "* System simulations: 693\n",
            "Время t:\n",
            " [ 0.          1.34888889  2.69777778  4.04666667  5.39555556  6.74444444\n",
            "  8.09333333  9.44222222 10.79111111 12.14      ]\n",
            "Функция g(х,y):\n",
            " [[ 0.00000000e+00  1.47576270e+01  2.95097620e+01  4.42557123e+01\n",
            "   5.90032412e+01  7.37580046e+01  8.85145050e+01  1.03271324e+02\n",
            "   1.18020796e+02  1.32740718e+02]\n",
            " [-2.00000000e+00 -1.83716668e+00 -1.33920381e+00 -7.21619358e-01\n",
            "  -1.59852293e-01  1.56249815e-01  3.56077799e-01  5.01088824e-01\n",
            "   9.67984150e-01  2.00346561e+00]\n",
            " [ 0.00000000e+00  2.34946105e-02  4.03465583e-02  4.23755271e-02\n",
            "   2.98501976e-02  1.63593982e-02  9.33213174e-03  1.65362469e-02\n",
            "   4.90186602e-02  9.29125539e-02]]\n",
            "Функция u(скорость, угол):\n",
            " [[ 1.09413755e+01  1.09419106e+01  1.09414488e+01  1.09413525e+01\n",
            "   1.09409246e+01  1.09407626e+01  1.09405797e+01  1.09405225e+01\n",
            "   1.09404652e+01  1.09404855e+01]\n",
            " [ 3.80443368e-03  5.81805311e-03  8.76031705e-04  1.54404102e-05\n",
            "  -5.11460662e-03 -6.98676601e-04 -2.13240617e-03  5.14152267e-03\n",
            "   7.99905895e-03  9.82704335e-03]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# КОД из ноутбука VIS_1\n",
        "@author: Vadim"
      ],
      "metadata": {
        "id": "aYbUGzK0xW2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install box2d-py"
      ],
      "metadata": {
        "id": "LYyqEGQKDJak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#изменение исходного кода ноутбука VIS_1\n",
        "установка дополнительных библиотек, позволяющих корректно установить OPENAI GYM в Google Colab\n",
        "\n",
        "полная документация по настройке библиотеки OpenAI GYM:\n",
        "http://gym.openai.com/docs/\n",
        "\n",
        "ПС:\n",
        "- рекомендация с сайта - https://stackoverflow.com/questions/49203023/openai-gym-nameerror-in-google-colaboratory\n",
        "\n",
        "В результате настроить OPENAI GYM не получилось; **возвращается ошибка:**\n",
        "\n",
        "NameError                                 Traceback (most recent call last)\n",
        "<ipython-input-6-36f0eabe77de> in <module>()\n",
        "      3 print(gym.__version__)# for me: 0.15.4\n",
        "      4 env = gym.make('CarRacing-v0')\n",
        "----> 5 env.reset()\n",
        "      6 for _ in range(1000):\n",
        "      7     env.render()\n",
        "\n",
        "NameError: name 'base' is not defined\n",
        "\n",
        "- еще вариант настройки, который опробован\n",
        "https://stackoverflow.com/questions/53472940/nameerror-name-base-is-not-defined-openai-gym"
      ],
      "metadata": {
        "id": "E99CW-v58DFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "#import\n",
        "import pyvirtualdisplay\n",
        "_display = pyvirtualdisplay.Display(visible=0,  # remember to use visible=0 and not False\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()\n",
        "\n",
        "#check\n",
        "!echo $DISPLAY"
      ],
      "metadata": {
        "id": "pOrroJ6N6mT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "print(gym.__version__)# for me: 0.15.4\n",
        "env = gym.make('CarRacing-v0')\n",
        "env.reset()\n",
        "for _ in range(1000):\n",
        "    env.render()\n",
        "    env.step(env.action_space.sample())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "63N3wxqhF8R5",
        "outputId": "a183cdbe-88e7-4079-96bb-54dd4518d2b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.17.3\n",
            "Track generation: 1080..1354 -> 274-tiles track\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NoSuchDisplayException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-36f0eabe77de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# for me: 0.15.4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CarRacing-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"state_pixels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/box2d/car_racing.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"state_pixels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWINDOW_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWINDOW_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модель (настройка TensorFlow)"
      ],
      "metadata": {
        "id": "JzjSjRSes0-q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc2HAOLhwS9o"
      },
      "source": [
        "# Разбиваем датафрейм на части, необходимые для обучения и тестирования модели  \n",
        "# x - данные с информацией о (x, y), у - целевая переменная (u) \n",
        "# x - вход в модель; y - выход модели\n",
        "\n",
        "# y[0], y[1] - x и y\n",
        "# u[0], u[1] - скорость и угловая скорость(ориентация автомобиля)\n",
        "# t - время t\n",
        "\n",
        "x = np.array(u)\n",
        "y = np.array(t)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN01uAu_xYQw",
        "outputId": "ecfb6bde-5ec1-47fd-d7c2-7cd857ebcd22"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.          1.34888889  2.69777778  4.04666667  5.39555556  6.74444444\n",
            "  8.09333333  9.44222222 10.79111111 12.14      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h8BJNebyduY",
        "outputId": "fe82c942-c9f9-4147-9559-18c90948488e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.09413755e+01  1.09419106e+01  1.09414488e+01  1.09413525e+01\n",
            "   1.09409246e+01  1.09407626e+01  1.09405797e+01  1.09405225e+01\n",
            "   1.09404652e+01  1.09404855e+01]\n",
            " [ 3.80443368e-03  5.81805311e-03  8.76031705e-04  1.54404102e-05\n",
            "  -5.11460662e-03 -6.98676601e-04 -2.13240617e-03  5.14152267e-03\n",
            "   7.99905895e-03  9.82704335e-03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "wcUBK0EPxdhi",
        "outputId": "8620d3d3-ad3f-48a6-ac80-92ce86e7b146"
      },
      "source": [
        "#строим и обучаем модель\n",
        "#используем Activation = relu - получается выше точность\n",
        "# об Activation - https://faroit.com/keras-docs/1.2.0/activations/ \n",
        "# * при количестве слоев: 20 и 20 и 1 и 100 - 0,1424-0,1426\n",
        "# при количестве слоев: 50 и 20 - 0,1416-0,1420\n",
        "# при количестве слоев: 20 и 50 - 0,1416-0,1420\n",
        "# при количестве слоев: 50 и 50 и 3 - 0,1415-0,1420\n",
        "# при количестве слоев: 20 и 20 и 3 - 0,1415-0,1420\n",
        "# при количестве слоев: 20 и 10 и 3 и 50 - 0,1416-0,1419\n",
        "# при количестве слоев: 20 и 20 и 10 и 1 и 50 - 0,1423-0,1425\n",
        "\n",
        "model = keras.Sequential([Dense(units=20, input_shape=(10,), activation='relu'), \n",
        "                          Dense(units=20, activation='sigmoid'),\n",
        "                          Dense(units=30, activation='sigmoid'),\n",
        "#                          Dense(units=10, activation='sigmoid'),                          \n",
        "                          Dense(units=2, activation='sigmoid')\n",
        "                         ])\n",
        "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(0.1))\n",
        "log = model.fit(x[0], y, epochs=50, verbose=False)\n",
        "plt.plot(log.history['loss'])\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name='dense_132_input'), name='dense_132_input', description=\"created by layer 'dense_132_input'\"), but it was called on an input with incompatible shape (None,).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-7ac709913f3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                          ])\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 227, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_33\" (type Sequential).\n    \n    Input 0 of layer \"dense_132\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02f79d7f"
      },
      "source": [
        "d=model.predict(x)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96aa8260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acdc8672-d3b0-4298-ace8-be1f7c3d8d8b"
      },
      "source": [
        "print(d)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.60006803 0.33666056]\n",
            " [0.58648705 0.33601886]]\n"
          ]
        }
      ]
    }
  ]
}